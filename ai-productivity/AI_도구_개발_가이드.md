# AI ë„êµ¬ ê°œë°œ ê°€ì´ë“œ
> AIë¥¼ í™œìš©í•œ ê¸°íšì ë° ê°œë°œììš© ìƒì‚°ì„± ë„êµ¬ êµ¬ì¶• ì‹¤ì „ ê°€ì´ë“œ

---

## ëª©ì°¨
1. [ê°œìš” ë° ëª©í‘œ](#1-ê°œìš”-ë°-ëª©í‘œ)
2. [AI ë„êµ¬ ê°œë°œ íŠ¸ë Œë“œ](#2-ai-ë„êµ¬-ê°œë°œ-íŠ¸ë Œë“œ)
3. [Phase 1: ê¸°ì´ˆ ì´í•´ ë° ì¤€ë¹„](#3-phase-1-ê¸°ì´ˆ-ì´í•´-ë°-ì¤€ë¹„)
4. [Phase 2: ì‹¤ì œ ì‚¬ë¡€ ë¶„ì„](#4-phase-2-ì‹¤ì œ-ì‚¬ë¡€-ë¶„ì„)
5. [Phase 3: ê¸°íšììš© ë„êµ¬ êµ¬ì¶•](#5-phase-3-ê¸°íšììš©-ë„êµ¬-êµ¬ì¶•)
6. [Phase 4: ê°œë°œììš© ë„êµ¬ êµ¬ì¶•](#6-phase-4-ê°œë°œììš©-ë„êµ¬-êµ¬ì¶•)
7. [Phase 5: ì‹¤ì „ êµ¬ì¶• ë° ë°°í¬](#7-phase-5-ì‹¤ì „-êµ¬ì¶•-ë°-ë°°í¬)
8. [ì‹¤ìŠµ ì˜ˆì œ](#8-ì‹¤ìŠµ-ì˜ˆì œ)
9. [ë¶€ë¡](#9-ë¶€ë¡)

---

## 1. ê°œìš” ë° ëª©í‘œ

### 1.1 ê°€ì´ë“œ ëª©í‘œ â­

ì´ ê°€ì´ë“œëŠ” **ê¸°íšìì™€ ê°œë°œìê°€ AIë¥¼ í™œìš©í•˜ì—¬ ì‹¤ë¬´ ìƒì‚°ì„± ë„êµ¬ë¥¼ ì§ì ‘ êµ¬ì¶•**í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤.

**ì£¼ìš” ëª©í‘œ:**
- AI API(OpenAI, Claude, Gemini)ë¥¼ í™œìš©í•œ ì‹¤ì „ ë„êµ¬ ê°œë°œ
- ê¸°íšììš©: ë¬¸ì„œ ìë™í™”, PRD ìƒì„±, íšŒì˜ë¡ ì •ë¦¬
- ê°œë°œììš©: ì½”ë“œ ë¦¬ë·°, í…ŒìŠ¤íŠ¸ ìƒì„±, ë¬¸ì„œí™” ìë™í™”
- ì‹¤ì œ ì„±ê³µ ì‚¬ë¡€ ë¶„ì„ ë° íŒ¨í„´ í•™ìŠµ
- ë¹„ìš© íš¨ìœ¨ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1.2 ëŒ€ìƒ ë…ì

**ê¸°íšì:**
- ë¬¸ì„œ ì‘ì„± ìë™í™”ë¥¼ ì›í•˜ëŠ” PM/PO
- AI ë„êµ¬ë¡œ ì—…ë¬´ íš¨ìœ¨ì„ ë†’ì´ê³  ì‹¶ì€ ê¸°íšì
- ê¸°ìˆ ì  ë°°ê²½ì´ ì—†ì–´ë„ Python ê¸°ë³¸ ë¬¸ë²•ë§Œ ì´í•´í•˜ë©´ ê°€ëŠ¥

**ê°œë°œì:**
- AIë¥¼ í™œìš©í•œ ì½”ë“œ ìë™í™” ë„êµ¬ë¥¼ ë§Œë“¤ê³  ì‹¶ì€ ê°œë°œì
- CI/CDì— AI ê¸°ëŠ¥ì„ í†µí•©í•˜ë ¤ëŠ” DevOps ì—”ì§€ë‹ˆì–´
- íŒ€ ìƒì‚°ì„± í–¥ìƒì„ ìœ„í•œ ë„êµ¬ë¥¼ êµ¬ì¶•í•˜ë ¤ëŠ” í…Œí¬ ë¦¬ë“œ

### 1.3 í•™ìŠµ ê²½ë¡œ

```
Phase 1: ê¸°ì´ˆ ì´í•´
   â†“
Phase 2: ì‚¬ë¡€ ë¶„ì„ â†’ ì„±ê³µ íŒ¨í„´ í•™ìŠµ
   â†“
Phase 3: ê¸°íšì ë„êµ¬ â†’ PRD, ë¬¸ì„œ ìë™í™”
   â†“
Phase 4: ê°œë°œì ë„êµ¬ â†’ ì½”ë“œ ë¦¬ë·°, í…ŒìŠ¤íŠ¸
   â†“
Phase 5: ì‹¤ì „ ë°°í¬ â†’ í”„ë¡œë•ì…˜ ì ìš©
```

### 1.4 í•„ìš”í•œ ì‚¬ì „ ì§€ì‹

**ìµœì†Œ ìš”êµ¬ì‚¬í•­:**
- Python ê¸°ë³¸ ë¬¸ë²• (ë³€ìˆ˜, í•¨ìˆ˜, ë°˜ë³µë¬¸)
- REST API ê°œë… ì´í•´
- JSON ë°ì´í„° êµ¬ì¡° ì´í•´
- í„°ë¯¸ë„ ëª…ë ¹ì–´ ê¸°ë³¸ ì‚¬ìš©ë²•

**ê¶Œì¥ ì‚¬í•­:**
- Git/GitHub ì‚¬ìš© ê²½í—˜
- í™˜ê²½ ë³€ìˆ˜ ì„¤ì • ê²½í—˜
- í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ê¸°ë³¸ ì´í•´

---

## 2. AI ë„êµ¬ ê°œë°œ íŠ¸ë Œë“œ

### 2.1 AI ë„êµ¬ ì±„íƒ í˜„í™© ğŸ“Š

**ìµœì‹  í†µê³„ (2025ë…„ ê¸°ì¤€):**
- **55%**: AI ë„êµ¬ê°€ ê¸°ëŒ€ì¹˜ë¥¼ ì´ˆê³¼í•˜ëŠ” ì„±ê³¼ ë‹¬ì„±
- **70%**: ì½”ë“œ ë° ë¬¸ì„œ í’ˆì§ˆ í–¥ìƒ ë³´ê³ 
- **4-5ë°°**: í‰ê·  ìƒì‚°ì„± í–¥ìƒ (ì‹¤ì œ ì‚¬ë¡€ ê¸°ì¤€)
- **75%**: ë¬¸ì„œ ì‘ì„± ì‹œê°„ ì ˆê° (PRD, íšŒì˜ë¡ ë“±)

### 2.2 ì£¼ìš” í™œìš© ë¶„ì•¼

#### 2.2.1 ê¸°íš/ë¬¸ì„œí™” ì˜ì—­
- **PRD(Product Requirement Document) ìƒì„±**
  - ìš”êµ¬ì‚¬í•­ ìë™ ì •ë¦¬ ë° êµ¬ì¡°í™”
  - ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìë™ ìƒì„±
  - ì˜ˆìƒ ì†Œìš” ì‹œê°„: 75% ì ˆê°

- **íšŒì˜ë¡ ë° ìš”ì•½**
  - ìŒì„±/í…ìŠ¤íŠ¸ íšŒì˜ë¡ ìë™ ì •ë¦¬
  - ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ
  - ë‹¤êµ­ì–´ ë²ˆì—­ ë° ìš”ì•½

- **ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±**
  - API ë¬¸ì„œ ìë™ ìƒì„±
  - ì‚¬ìš©ì ê°€ì´ë“œ ì‘ì„±
  - FAQ ìë™ ìƒì„±

#### 2.2.2 ê°œë°œ/ìë™í™” ì˜ì—­
- **ì½”ë“œ ë¦¬ë·° ìë™í™”**
  - ì½”ë“œ í’ˆì§ˆ ë¶„ì„
  - ë³´ì•ˆ ì·¨ì•½ì  íƒì§€
  - ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ì œì•ˆ

- **í…ŒìŠ¤íŠ¸ ì½”ë“œ ìƒì„±**
  - Unit Test ìë™ ìƒì„±
  - Edge Case ì‹ë³„
  - í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ í–¥ìƒ

- **ë¬¸ì„œí™” ìë™í™”**
  - ì½”ë“œ ì£¼ì„ ìë™ ìƒì„±
  - README ì‘ì„±
  - API ë¬¸ì„œ ë™ê¸°í™”

### 2.3 ì£¼ìš” AI ì œê³µì ë¹„êµ

| ì œê³µì | ê°•ì  | ìµœì  ìš©ë„ | ê°€ê²© (ì…ë ¥/ì¶œë ¥) |
|--------|------|-----------|------------------|
| **OpenAI GPT-4** | ë²”ìš©ì„±, ìƒíƒœê³„ | ë¬¸ì„œ ìƒì„±, ëŒ€í™”í˜• ë„êµ¬ | $0.03/$0.06 (1K tokens) |
| **Anthropic Claude** | ê¸´ ì»¨í…ìŠ¤íŠ¸, ì •í™•ë„ | ì½”ë“œ ë¶„ì„, ë³µì¡í•œ ë¬¸ì„œ | $0.03/$0.15 (1K tokens) |
| **Google Gemini** | ë©€í‹°ëª¨ë‹¬, í†µí•©ì„± | ë°ì´í„° ë¶„ì„, ì´ë¯¸ì§€ ì²˜ë¦¬ | $0.025/$0.05 (1K tokens) |
| **Azure OpenAI** | ì—”í„°í”„ë¼ì´ì¦ˆ, ë³´ì•ˆ | ê¸°ì—…ìš© ì†”ë£¨ì…˜ | Custom pricing |

### 2.4 ì„±ê³µì ì¸ AI ë„êµ¬ì˜ íŠ¹ì§• âœ…

**1. ëª…í™•í•œ ë¬¸ì œ ì •ì˜**
- í•´ê²°í•˜ë ¤ëŠ” ë¬¸ì œê°€ êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥
- ì‚¬ìš©ì í˜ì¸ í¬ì¸íŠ¸ë¥¼ ì •í™•íˆ íŒŒì•…
- ROIë¥¼ ëª…í™•íˆ ì œì‹œ

**2. ì ì ˆí•œ AI ëª¨ë¸ ì„ íƒ**
- ê³¼ì—…ì˜ ë³µì¡ë„ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ
- ë¹„ìš© ëŒ€ë¹„ ì„±ëŠ¥ ìµœì í™”
- ì‘ë‹µ ì†ë„ì™€ ì •í™•ë„ ê· í˜•

**3. ì‚¬ìš©ì ê²½í—˜ ìµœìš°ì„ **
- ê°„ë‹¨í•˜ê³  ì§ê´€ì ì¸ ì¸í„°í˜ì´ìŠ¤
- ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„ (< 5ì´ˆ)
- ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ë° ê°€ì´ë“œ

**4. ë°˜ë³µì  ê°œì„ **
- ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘ ì²´ê³„
- A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•œ í”„ë¡¬í”„íŠ¸ ìµœì í™”
- ì§€ì†ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

---

## 3. Phase 1: ê¸°ì´ˆ ì´í•´ ë° ì¤€ë¹„

### 3.1 AI API ê¸°ë³¸ êµ¬ì¡° ì´í•´

#### 3.1.1 ê³µí†µ API íŒ¨í„´

ëª¨ë“  ì£¼ìš” AI ì œê³µìëŠ” ìœ ì‚¬í•œ API êµ¬ì¡°ë¥¼ ë”°ë¦…ë‹ˆë‹¤:

```python
# ê³µí†µ íŒ¨í„´
response = ai_client.chat.completions.create(
    model="model-name",
    messages=[
        {"role": "system", "content": "ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­"},
        {"role": "user", "content": "ì‚¬ìš©ì ì…ë ¥"}
    ],
    temperature=0.7,  # ì°½ì˜ì„± ì¡°ì ˆ (0-1)
    max_tokens=1000   # ìµœëŒ€ ì‘ë‹µ ê¸¸ì´
)
```

#### 3.1.2 ì£¼ìš” íŒŒë¼ë¯¸í„° ì´í•´

**1. temperature (ì°½ì˜ì„± ì¡°ì ˆ)**
- `0.0-0.3`: ê²°ì •ì , ì¼ê´€ì„± ì¤‘ìš” (ì½”ë“œ ìƒì„±, ë°ì´í„° ì¶”ì¶œ)
- `0.4-0.7`: ê· í˜• (ë¬¸ì„œ ì‘ì„±, ìš”ì•½)
- `0.8-1.0`: ì°½ì˜ì  (ì•„ì´ë””ì–´ ìƒì„±, ë¸Œë ˆì¸ìŠ¤í† ë°)

**2. max_tokens (ì‘ë‹µ ê¸¸ì´)**
- ì§§ì€ ë‹µë³€: 100-500 tokens
- ì¤‘ê°„ ë¬¸ì„œ: 500-2000 tokens
- ê¸´ ë¬¸ì„œ: 2000-4000 tokens
- âš ï¸ ë¹„ìš©ê³¼ ì§ê²°ë˜ë¯€ë¡œ í•„ìš”í•œ ë§Œí¼ë§Œ ì„¤ì •

**3. system message (ì—­í•  ì •ì˜)**
```python
# ì¢‹ì€ ì˜ˆì‹œ
system_message = """ë‹¹ì‹ ì€ ì „ë¬¸ ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±ìì…ë‹ˆë‹¤.
ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥´ì„¸ìš”:
- ëª…í™•í•˜ê³  ê°„ê²°í•œ ë¬¸ì¥ ì‚¬ìš©
- ê¸°ìˆ  ìš©ì–´ëŠ” ì²« ì‚¬ìš© ì‹œ ì„¤ëª…
- ì½”ë“œ ì˜ˆì‹œëŠ” ì£¼ì„ í¬í•¨
- 3ë‹¨ê³„ êµ¬ì¡°: ê°œìš” â†’ ìƒì„¸ â†’ ì˜ˆì‹œ"""

# ë‚˜ìœ ì˜ˆì‹œ
system_message = "ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”."  # ë„ˆë¬´ ëª¨í˜¸í•¨
```

### 3.2 ê°œë°œ í™˜ê²½ ì„¤ì •

#### 3.2.1 Python í™˜ê²½ êµ¬ì„±

**1. ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)**
```bash
# Python 3.8+ í•„ìš”
python -m venv ai-tools-env

# í™œì„±í™” (macOS/Linux)
source ai-tools-env/bin/activate

# í™œì„±í™” (Windows)
ai-tools-env\Scripts\activate
```

**2. í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**
```bash
# requirements.txt ìƒì„±
cat > requirements.txt << EOF
openai>=1.0.0
anthropic>=0.7.0
google-generativeai>=0.3.0
python-dotenv>=1.0.0
requests>=2.31.0
pydantic>=2.0.0
EOF

# ì„¤ì¹˜
pip install -r requirements.txt
```

#### 3.2.2 API í‚¤ ì„¤ì •

**1. .env íŒŒì¼ ìƒì„± (ë³´ì•ˆ ì¤‘ìš”!) âš ï¸**
```bash
# .env íŒŒì¼
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AI...

# âš ï¸ .gitignoreì— ë°˜ë“œì‹œ ì¶”ê°€!
echo ".env" >> .gitignore
```

**2. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ**
```python
# config.py
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

    @classmethod
    def validate(cls):
        """API í‚¤ ìœ íš¨ì„± ê²€ì‚¬"""
        if not cls.OPENAI_API_KEY:
            raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        # í•„ìš”í•œ í‚¤ë§Œ ê²€ì‚¬
```

#### 3.2.3 ì²« ë²ˆì§¸ AI í˜¸ì¶œ í…ŒìŠ¤íŠ¸

**OpenAI ê¸°ë³¸ í…ŒìŠ¤íŠ¸:**
```python
# test_openai.py
from openai import OpenAI
from config import Config

Config.validate()
client = OpenAI(api_key=Config.OPENAI_API_KEY)

def test_basic_call():
    """ê¸°ë³¸ AI í˜¸ì¶œ í…ŒìŠ¤íŠ¸"""
    response = client.chat.completions.create(
        model="gpt-4",
        messages=[
            {"role": "system", "content": "ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”."},
            {"role": "user", "content": "AI API í…ŒìŠ¤íŠ¸ ì„±ê³µ ì—¬ë¶€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”."}
        ],
        max_tokens=50
    )

    print("âœ… API ì—°ê²° ì„±ê³µ!")
    print(f"ì‘ë‹µ: {response.choices[0].message.content}")
    print(f"ì‚¬ìš© í† í°: {response.usage.total_tokens}")

if __name__ == "__main__":
    test_basic_call()
```

**Claude ê¸°ë³¸ í…ŒìŠ¤íŠ¸:**
```python
# test_claude.py
from anthropic import Anthropic
from config import Config

Config.validate()
client = Anthropic(api_key=Config.ANTHROPIC_API_KEY)

def test_claude_call():
    """Claude API í…ŒìŠ¤íŠ¸"""
    response = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=100,
        messages=[
            {"role": "user", "content": "Claude API í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤."}
        ]
    )

    print("âœ… Claude API ì—°ê²° ì„±ê³µ!")
    print(f"ì‘ë‹µ: {response.content[0].text}")

if __name__ == "__main__":
    test_claude_call()
```

**Gemini ê¸°ë³¸ í…ŒìŠ¤íŠ¸:**
```python
# test_gemini.py
import google.generativeai as genai
from config import Config

Config.validate()
genai.configure(api_key=Config.GOOGLE_API_KEY)

def test_gemini_call():
    """Gemini API í…ŒìŠ¤íŠ¸"""
    model = genai.GenerativeModel('gemini-pro')
    response = model.generate_content("Gemini API í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤.")

    print("âœ… Gemini API ì—°ê²° ì„±ê³µ!")
    print(f"ì‘ë‹µ: {response.text}")

if __name__ == "__main__":
    test_gemini_call()
```

### 3.3 í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ

#### 3.3.1 íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°

**CRISP í”„ë ˆì„ì›Œí¬:**
```
C - Context (ë§¥ë½): ìƒí™© ì„¤ëª…
R - Role (ì—­í• ): AIì˜ ì—­í•  ì •ì˜
I - Instruction (ì§€ì‹œ): êµ¬ì²´ì  ì‘ì—…
S - Specifics (ì„¸ë¶€ì‚¬í•­): ì œì•½ì¡°ê±´, í˜•ì‹
P - Purpose (ëª©ì ): ìµœì¢… ëª©í‘œ
```

**ì‹¤ì œ ì˜ˆì‹œ:**
```python
prompt = """
[Context] ìš°ë¦¬ íŒ€ì€ ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ì‹ ê·œ ê¸°ëŠ¥ì„ ê°œë°œ ì¤‘ì…ë‹ˆë‹¤.

[Role] ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ PMì…ë‹ˆë‹¤.

[Instruction] ë‹¤ìŒ ê¸°ëŠ¥ì— ëŒ€í•œ PRDë¥¼ ì‘ì„±í•˜ì„¸ìš”:
- ê¸°ëŠ¥ëª…: ì‚¬ìš©ì ì•Œë¦¼ ì„¤ì •
- ëª©ì : ì‚¬ìš©ìê°€ ì•Œë¦¼ ìˆ˜ì‹  ë°©ì‹ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•

[Specifics]
- í˜•ì‹: ì œëª©, ê°œìš”, ìš”êµ¬ì‚¬í•­, ì„±ê³µì§€í‘œ
- ê¸¸ì´: 500ë‹¨ì–´ ì´ë‚´
- ê¸°ìˆ  ìŠ¤íƒ: React, Node.js, MongoDB

[Purpose] ê°œë°œíŒ€ì´ ë°”ë¡œ ì°©ìˆ˜í•  ìˆ˜ ìˆë„ë¡ ëª…í™•í•œ ìš”êµ¬ì‚¬í•­ ì œì‹œ
"""
```

#### 3.3.2 Few-Shot Learning (ì˜ˆì‹œ ì œê³µ)

**Zero-Shot (ì˜ˆì‹œ ì—†ìŒ):**
```python
# íš¨ê³¼ ë‚®ìŒ
prompt = "ì´ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”: [ì½”ë“œ]"
```

**Few-Shot (ì˜ˆì‹œ í¬í•¨):**
```python
# íš¨ê³¼ ë†’ìŒ
prompt = """
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì½”ë“œë¥¼ ë¦¬ë·°í•´ì£¼ì„¸ìš”:

ì˜ˆì‹œ 1:
ì½”ë“œ: function add(a, b) { return a + b }
ë¦¬ë·°: âœ… ê°„ê²°í•˜ê³  ëª…í™•í•¨. íƒ€ì… ê²€ì¦ ì¶”ê°€ ê¶Œì¥.

ì˜ˆì‹œ 2:
ì½”ë“œ: var x = 1; var y = 2; var z = x + y;
ë¦¬ë·°: âš ï¸ let/const ì‚¬ìš© ê¶Œì¥. ë³€ìˆ˜ëª…ì„ ì˜ë¯¸ìˆê²Œ ë³€ê²½.

ì´ì œ ë‹¤ìŒ ì½”ë“œë¥¼ ë¦¬ë·°í•˜ì„¸ìš”:
[ì‹¤ì œ ì½”ë“œ]
"""
```

#### 3.3.3 Chain-of-Thought (ì‚¬ê³  ê³¼ì • ìœ ë„)

**ì¼ë°˜ í”„ë¡¬í”„íŠ¸:**
```python
# ì •í™•ë„ ë‚®ìŒ
prompt = "ì´ ë²„ê·¸ì˜ ì›ì¸ì„ ì°¾ìœ¼ì„¸ìš”."
```

**CoT í”„ë¡¬í”„íŠ¸:**
```python
# ì •í™•ë„ ë†’ìŒ
prompt = """
ë‹¤ìŒ ë‹¨ê³„ë¡œ ë²„ê·¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”:

1. ì—ëŸ¬ ë©”ì‹œì§€ ë¶„ì„
2. ê´€ë ¨ ì½”ë“œ ì„¹ì…˜ ì‹ë³„
3. ê°€ëŠ¥í•œ ì›ì¸ 3ê°€ì§€ ë‚˜ì—´
4. ê° ì›ì¸ì˜ ê°€ëŠ¥ì„± í‰ê°€
5. ìµœì¢… ì›ì¸ ê²°ë¡  ë° ìˆ˜ì • ë°©ì•ˆ

[ë²„ê·¸ ì •ë³´]
"""
```

### 3.4 ë¹„ìš© ìµœì í™” ì „ëµ ğŸ’¡

#### 3.4.1 í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

```python
# token_monitor.py
import tiktoken

def count_tokens(text: str, model: str = "gpt-4") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))

def estimate_cost(prompt: str, max_tokens: int, model: str = "gpt-4") -> float:
    """ì˜ˆìƒ ë¹„ìš© ê³„ì‚°"""
    input_tokens = count_tokens(prompt, model)

    # ê°€ê²© (2025ë…„ ê¸°ì¤€, 1K tokensë‹¹)
    prices = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.001, "output": 0.002},
        "claude-3-opus": {"input": 0.015, "output": 0.075},
    }

    price = prices.get(model, prices["gpt-4"])
    input_cost = (input_tokens / 1000) * price["input"]
    output_cost = (max_tokens / 1000) * price["output"]

    return input_cost + output_cost

# ì‚¬ìš© ì˜ˆì‹œ
prompt = "ê¸´ í”„ë¡¬í”„íŠ¸ ë‚´ìš©..."
print(f"ì˜ˆìƒ ë¹„ìš©: ${estimate_cost(prompt, 1000):.4f}")
```

#### 3.4.2 ë¹„ìš© ì ˆê° íŒ

**1. ì ì ˆí•œ ëª¨ë¸ ì„ íƒ**
```python
# ê°„ë‹¨í•œ ì‘ì—… â†’ ì €ë ´í•œ ëª¨ë¸
simple_tasks = ["ìš”ì•½", "ë¶„ë¥˜", "í‚¤ì›Œë“œ ì¶”ì¶œ"]
if task in simple_tasks:
    model = "gpt-3.5-turbo"  # 30ë°° ì €ë ´
else:
    model = "gpt-4"
```

**2. ìºì‹± í™œìš©**
```python
# cache.py
import json
import hashlib

class ResponseCache:
    def __init__(self, cache_file="cache.json"):
        self.cache_file = cache_file
        self.cache = self._load_cache()

    def _load_cache(self):
        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {}

    def get_cache_key(self, prompt: str, model: str) -> str:
        """í”„ë¡¬í”„íŠ¸ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        content = f"{model}:{prompt}"
        return hashlib.md5(content.encode()).hexdigest()

    def get(self, prompt: str, model: str):
        """ìºì‹œì—ì„œ ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°"""
        key = self.get_cache_key(prompt, model)
        return self.cache.get(key)

    def set(self, prompt: str, model: str, response: str):
        """ìºì‹œì— ì‘ë‹µ ì €ì¥"""
        key = self.get_cache_key(prompt, model)
        self.cache[key] = response
        with open(self.cache_file, 'w') as f:
            json.dump(self.cache, f)
```

**3. ë°°ì¹˜ ì²˜ë¦¬**
```python
# ë¹„íš¨ìœ¨ì 
for item in items:
    response = ai_call(item)  # 100ë²ˆ í˜¸ì¶œ

# íš¨ìœ¨ì 
batch_prompt = "\n".join([f"{i}. {item}" for i, item in enumerate(items)])
response = ai_call(batch_prompt)  # 1ë²ˆ í˜¸ì¶œ
```

### 3.5 ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§

#### 3.5.1 ê¸°ë³¸ ì—ëŸ¬ ì²˜ë¦¬

```python
# error_handler.py
import time
from typing import Callable, Any
from openai import OpenAI, APIError, RateLimitError, APIConnectionError

def retry_with_exponential_backoff(
    func: Callable,
    max_retries: int = 3,
    initial_delay: float = 1.0,
    max_delay: float = 60.0
) -> Any:
    """ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„"""
    delay = initial_delay

    for attempt in range(max_retries):
        try:
            return func()
        except RateLimitError as e:
            print(f"âš ï¸ Rate limit ë„ë‹¬. {delay}ì´ˆ í›„ ì¬ì‹œë„... (ì‹œë„ {attempt + 1}/{max_retries})")
            time.sleep(delay)
            delay = min(delay * 2, max_delay)
        except APIConnectionError as e:
            print(f"âš ï¸ ì—°ê²° ì˜¤ë¥˜. {delay}ì´ˆ í›„ ì¬ì‹œë„...")
            time.sleep(delay)
            delay = min(delay * 2, max_delay)
        except APIError as e:
            print(f"âŒ API ì˜¤ë¥˜: {e}")
            raise

    raise Exception(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜({max_retries})ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.")

# ì‚¬ìš© ì˜ˆì‹œ
def safe_ai_call(prompt: str):
    client = OpenAI()

    def call():
        return client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )

    return retry_with_exponential_backoff(call)
```

#### 3.5.2 íƒ€ì„ì•„ì›ƒ ì„¤ì •

```python
# timeout_handler.py
import signal
from contextlib import contextmanager

class TimeoutError(Exception):
    pass

@contextmanager
def timeout(seconds: int):
    """íƒ€ì„ì•„ì›ƒ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    def handler(signum, frame):
        raise TimeoutError(f"{seconds}ì´ˆ íƒ€ì„ì•„ì›ƒ")

    # ì‹œê·¸ë„ ì„¤ì •
    signal.signal(signal.SIGALRM, handler)
    signal.alarm(seconds)

    try:
        yield
    finally:
        signal.alarm(0)

# ì‚¬ìš© ì˜ˆì‹œ
try:
    with timeout(30):  # 30ì´ˆ ì œí•œ
        response = ai_call(long_prompt)
except TimeoutError:
    print("âŒ AI ì‘ë‹µ ì‹œê°„ ì´ˆê³¼")
```

---

**âœ… Phase 1 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Python í™˜ê²½ ì„¤ì • ì™„ë£Œ
- [ ] API í‚¤ ë°œê¸‰ ë° ì„¤ì • ì™„ë£Œ
- [ ] ê¸°ë³¸ API í˜¸ì¶œ í…ŒìŠ¤íŠ¸ ì„±ê³µ
- [ ] í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ ì´í•´
- [ ] ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë„êµ¬ êµ¬í˜„
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ êµ¬í˜„

**ë‹¤ìŒ ë‹¨ê³„:** Phase 2ì—ì„œ ì‹¤ì œ ì„±ê³µ ì‚¬ë¡€ë¥¼ ë¶„ì„í•˜ë©° íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.

---

## 4. Phase 2: ì‹¤ì œ ì‚¬ë¡€ ë¶„ì„

### 4.1 ì‚¬ë¡€ 1: Gazelle - ì •í™•ë„ í˜ì‹  â­

**ê¸°ì—…:** ê¸€ë¡œë²Œ ë¬¼ë¥˜ íšŒì‚¬
**ë„ì „ ê³¼ì œ:** ì†¡ì¥ ì²˜ë¦¬ ì •í™•ë„ 95% â†’ 99.9% ê°œì„  í•„ìš”
**ì†”ë£¨ì…˜:** Gemini APIë¥¼ í™œìš©í•œ ë¬¸ì„œ ë°ì´í„° ì¶”ì¶œ ìë™í™”

#### 4.1.1 ë¬¸ì œ ì •ì˜

**Before (ìˆ˜ì‘ì—…):**
- ì†¡ì¥ ë°ì´í„° ì…ë ¥: ë¬¸ì„œë‹¹ 4ì‹œê°„ ì†Œìš”
- ì •í™•ë„: 95% (ì˜¤ë¥˜ìœ¨ 5%)
- ì›” ì²˜ë¦¬ëŸ‰: 1,000ê±´
- ì¸ë ¥: 10ëª…ì˜ ë°ì´í„° ì…ë ¥ ë‹´ë‹¹ì

**ë¬¸ì œì :**
- ìˆ˜ì‘ì—…ìœ¼ë¡œ ì¸í•œ íœ´ë¨¼ ì—ëŸ¬
- ì²˜ë¦¬ ì‹œê°„ ê³¼ë‹¤ë¡œ ë³‘ëª© í˜„ìƒ
- í™•ì¥ì„± ë¶€ì¡± (ë¬¼ëŸ‰ ì¦ê°€ ì‹œ ì¸ë ¥ ì¶”ê°€ í•„ìš”)

#### 4.1.2 AI ì†”ë£¨ì…˜ ì„¤ê³„

**ê¸°ìˆ  ìŠ¤íƒ:**
- **AI ëª¨ë¸:** Google Gemini Pro Vision (ë©€í‹°ëª¨ë‹¬)
- **ì–¸ì–´:** Python
- **ì¸í”„ë¼:** Google Cloud Run (ì„œë²„ë¦¬ìŠ¤)
- **ë°ì´í„°ë² ì´ìŠ¤:** Firestore

**ì•„í‚¤í…ì²˜:**
```
PDF/ì´ë¯¸ì§€ ì†¡ì¥
    â†“
Gemini Vision API (OCR + ë°ì´í„° ì¶”ì¶œ)
    â†“
êµ¬ì¡°í™”ëœ JSON ë°ì´í„°
    â†“
ê²€ì¦ ë¡œì§ (ë£° ê¸°ë°˜)
    â†“
Firestore ì €ì¥
```

#### 4.1.3 í•µì‹¬ êµ¬í˜„ ì½”ë“œ

```python
# gazelle_invoice_processor.py
import google.generativeai as genai
from typing import Dict, List
import json
from dataclasses import dataclass
from datetime import datetime

@dataclass
class InvoiceData:
    """ì†¡ì¥ ë°ì´í„° êµ¬ì¡°"""
    invoice_number: str
    date: str
    supplier: str
    items: List[Dict[str, any]]
    total_amount: float
    currency: str

class GazelleInvoiceProcessor:
    """Gazelle ì†¡ì¥ ì²˜ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self, api_key: str):
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro-vision')

    def extract_invoice_data(self, image_path: str) -> InvoiceData:
        """
        ì†¡ì¥ ì´ë¯¸ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ

        í•µì‹¬: êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸ + Few-Shot ì˜ˆì‹œ
        """
        # ì´ë¯¸ì§€ ë¡œë“œ
        import PIL.Image
        image = PIL.Image.open(image_path)

        # êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸
        prompt = """
ë‹¤ìŒ ì†¡ì¥ ì´ë¯¸ì§€ì—ì„œ ì •í™•í•˜ê²Œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¶œë ¥ í˜•ì‹ (JSON):**
{
  "invoice_number": "ì†¡ì¥ë²ˆí˜¸",
  "date": "YYYY-MM-DD",
  "supplier": "ê³µê¸‰ì—…ì²´ëª…",
  "items": [
    {
      "description": "í’ˆëª© ì„¤ëª…",
      "quantity": ìˆ˜ëŸ‰,
      "unit_price": ë‹¨ê°€,
      "total": ê¸ˆì•¡
    }
  ],
  "total_amount": ì´ì•¡,
  "currency": "í†µí™”ì½”ë“œ"
}

**ì¶”ì¶œ ê·œì¹™:**
1. ìˆ«ìëŠ” ì‰¼í‘œ ì œê±° í›„ ìˆ«ìë§Œ ì¶”ì¶œ
2. ë‚ ì§œëŠ” ISO 8601 í˜•ì‹ìœ¼ë¡œ í†µì¼
3. ê¸ˆì•¡ì€ ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ê¹Œì§€
4. ë¶ˆëª…í™•í•œ í•­ëª©ì€ "UNCLEAR"ë¡œ í‘œì‹œ

**ì˜ˆì‹œ:**
ì†¡ì¥ ì´ë¯¸ì§€ì— "Invoice #12345, Date: 01/15/2024"ê°€ ë³´ì´ë©´
â†’ {"invoice_number": "12345", "date": "2024-01-15"}
"""

        # AI í˜¸ì¶œ
        response = self.model.generate_content([prompt, image])

        # JSON íŒŒì‹±
        try:
            # ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ (```json ... ``` ì œê±°)
            json_text = response.text
            if "```json" in json_text:
                json_text = json_text.split("```json")[1].split("```")[0]

            data = json.loads(json_text.strip())
            return InvoiceData(**data)

        except json.JSONDecodeError as e:
            raise ValueError(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}\nì‘ë‹µ: {response.text}")

    def validate_invoice(self, invoice: InvoiceData) -> tuple[bool, List[str]]:
        """
        ì†¡ì¥ ë°ì´í„° ê²€ì¦

        ë£° ê¸°ë°˜ ê²€ì¦ìœ¼ë¡œ ì •í™•ë„ 99.9% ë‹¬ì„±
        """
        errors = []

        # 1. í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not invoice.invoice_number or invoice.invoice_number == "UNCLEAR":
            errors.append("ì†¡ì¥ë²ˆí˜¸ ëˆ„ë½")

        # 2. ë‚ ì§œ í˜•ì‹ ê²€ì¦
        try:
            datetime.strptime(invoice.date, "%Y-%m-%d")
        except ValueError:
            errors.append(f"ë‚ ì§œ í˜•ì‹ ì˜¤ë¥˜: {invoice.date}")

        # 3. ê¸ˆì•¡ ì¼ì¹˜ ê²€ì¦
        calculated_total = sum(item['total'] for item in invoice.items)
        if abs(calculated_total - invoice.total_amount) > 0.01:
            errors.append(
                f"ê¸ˆì•¡ ë¶ˆì¼ì¹˜: ê³„ì‚°ê°’ {calculated_total} â‰  ì´ì•¡ {invoice.total_amount}"
            )

        # 4. í†µí™” ì½”ë“œ ê²€ì¦
        valid_currencies = ["USD", "EUR", "KRW", "JPY", "CNY"]
        if invoice.currency not in valid_currencies:
            errors.append(f"ìœ íš¨í•˜ì§€ ì•Šì€ í†µí™”: {invoice.currency}")

        return len(errors) == 0, errors

    def process_invoice_with_retry(
        self,
        image_path: str,
        max_retries: int = 3
    ) -> Dict:
        """
        ì¬ì‹œë„ ë¡œì§ì„ í¬í•¨í•œ ì†¡ì¥ ì²˜ë¦¬

        ì •í™•ë„ í–¥ìƒì˜ í•µì‹¬: ê²€ì¦ ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ + í”¼ë“œë°±
        """
        for attempt in range(max_retries):
            try:
                # ë°ì´í„° ì¶”ì¶œ
                invoice = self.extract_invoice_data(image_path)

                # ê²€ì¦
                is_valid, errors = self.validate_invoice(invoice)

                if is_valid:
                    return {
                        "status": "success",
                        "data": invoice.__dict__,
                        "attempts": attempt + 1
                    }
                else:
                    print(f"âš ï¸ ê²€ì¦ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {errors}")

                    if attempt < max_retries - 1:
                        # í”¼ë“œë°±ì„ ì¶”ê°€í•˜ì—¬ ì¬ì‹œë„
                        # (ì‹¤ì œë¡œëŠ” í”„ë¡¬í”„íŠ¸ì— ì—ëŸ¬ ì •ë³´ ì¶”ê°€)
                        continue

            except Exception as e:
                print(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")

        # ìµœëŒ€ ì¬ì‹œë„ ì´ˆê³¼
        return {
            "status": "failed",
            "error": "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼",
            "attempts": max_retries
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    processor = GazelleInvoiceProcessor(api_key="YOUR_API_KEY")

    result = processor.process_invoice_with_retry("invoice_sample.pdf")

    if result["status"] == "success":
        print(f"âœ… ì²˜ë¦¬ ì„±ê³µ (ì‹œë„ íšŸìˆ˜: {result['attempts']})")
        print(json.dumps(result["data"], indent=2, ensure_ascii=False))
    else:
        print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {result['error']}")
```

#### 4.1.4 ì„±ê³¼ ë° êµí›ˆ

**ì •ëŸ‰ì  ì„±ê³¼:**
- âœ… ì •í™•ë„: 95% â†’ **99.9%** (5ë°° í–¥ìƒ)
- âœ… ì²˜ë¦¬ ì‹œê°„: 4ì‹œê°„ â†’ **10ì´ˆ** (1,440ë°° í–¥ìƒ)
- âœ… ë¹„ìš© ì ˆê°: ì¸ë ¥ 10ëª… â†’ 2ëª… (80% ê°ì†Œ)
- âœ… ì²˜ë¦¬ëŸ‰: 1,000ê±´/ì›” â†’ **10,000ê±´/ì›”** (10ë°° ì¦ê°€)

**í•µì‹¬ ì„±ê³µ ìš”ì¸:**
1. **ë©€í‹°ëª¨ë‹¬ AI í™œìš©**: Gemini Visionìœ¼ë¡œ ì´ë¯¸ì§€ ì§ì ‘ ì²˜ë¦¬
2. **êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸**: JSON ìŠ¤í‚¤ë§ˆë¥¼ ëª…í™•íˆ ì œì‹œ
3. **ê²€ì¦ + ì¬ì‹œë„**: ë£° ê¸°ë°˜ ê²€ì¦ìœ¼ë¡œ ì˜¤ë¥˜ ì œê±°
4. **ì ì§„ì  ê°œì„ **: ì´ˆê¸° 90% â†’ í”¼ë“œë°± ë°˜ì˜ â†’ 99.9%

**êµí›ˆ:**
- ğŸ’¡ ë‹¨ìˆœ OCRì´ ì•„ë‹Œ **ë§¥ë½ ì´í•´**ê°€ ì¤‘ìš” (AIì˜ ê°•ì )
- ğŸ’¡ ê²€ì¦ ë¡œì§ì´ ì •í™•ë„ì˜ í•µì‹¬ (AI + ë£° ê¸°ë°˜ ì¡°í•©)
- ğŸ’¡ ì¬ì‹œë„ ì „ëµìœ¼ë¡œ ì¼ì‹œì  ì˜¤ë¥˜ í•´ê²°

---

### 4.2 ì‚¬ë¡€ 2: Domina - ë°ì´í„° ì ‘ê·¼ì„± í˜ì‹  ğŸ“Š

**ê¸°ì—…:** ì¤‘ê²¬ ìœ í†µ íšŒì‚¬
**ë„ì „ ê³¼ì œ:** ë°ì´í„° ë¶„ì„ ì ‘ê·¼ì„± 80% í–¥ìƒ, ë°°ì†¡ íš¨ìœ¨ 15% ê°œì„ 
**ì†”ë£¨ì…˜:** Vertex AI ê¸°ë°˜ ìì—°ì–´ ì¿¼ë¦¬ ì‹œìŠ¤í…œ

#### 4.2.1 ë¬¸ì œ ì •ì˜

**Before:**
- ë°ì´í„° ë¶„ì„ ìš”ì²­ ì‹œ SQL ì‘ì„± í•„ìš” (ì „ë¬¸ê°€ ì˜ì¡´)
- ë¹„ì¦ˆë‹ˆìŠ¤ íŒ€ì´ ë°ì´í„° ì ‘ê·¼ ì–´ë ¤ì›€
- ë¶„ì„ ìš”ì²­ â†’ ê²°ê³¼ ìˆ˜ì‹ : í‰ê·  3ì¼ ì†Œìš”

**After:**
- ìì—°ì–´ë¡œ ì§ˆë¬¸ â†’ ì¦‰ì‹œ ê²°ê³¼ í™•ì¸
- ë¹„ê¸°ìˆ  íŒ€ì›ë„ ë°ì´í„° ë¶„ì„ ê°€ëŠ¥
- ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ê°€ëŠ¥

#### 4.2.2 AI ì†”ë£¨ì…˜ ì„¤ê³„

**ê¸°ìˆ  ìŠ¤íƒ:**
- **AI ëª¨ë¸:** Google Vertex AI (PaLM 2)
- **ì–¸ì–´:** Python + SQL
- **ë°ì´í„°ë² ì´ìŠ¤:** BigQuery
- **ì¸í„°í˜ì´ìŠ¤:** Streamlit ì›¹ ì•±

**ì•„í‚¤í…ì²˜:**
```
ì‚¬ìš©ì ìì—°ì–´ ì§ˆë¬¸
    â†“
Vertex AI (ìì—°ì–´ â†’ SQL ë³€í™˜)
    â†“
SQL ê²€ì¦ ë° ìµœì í™”
    â†“
BigQuery ì‹¤í–‰
    â†“
ê²°ê³¼ ì‹œê°í™” (ì°¨íŠ¸, í‘œ)
```

#### 4.2.3 í•µì‹¬ êµ¬í˜„ ì½”ë“œ

```python
# domina_nl_query_system.py
from google.cloud import aiplatform, bigquery
from typing import Dict, List, Any
import re

class DominaNLQuerySystem:
    """
    Domina ìì—°ì–´ ì¿¼ë¦¬ ì‹œìŠ¤í…œ

    í•µì‹¬: ìì—°ì–´ â†’ SQL ë³€í™˜ + ì•ˆì „ì„± ê²€ì¦
    """

    def __init__(self, project_id: str, location: str = "us-central1"):
        self.project_id = project_id
        aiplatform.init(project=project_id, location=location)
        self.bq_client = bigquery.Client(project=project_id)

        # ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ (AIì—ê²Œ ì œê³µ)
        self.schema_context = """
ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ:

1. orders í…Œì´ë¸”:
   - order_id (STRING): ì£¼ë¬¸ ID
   - customer_id (STRING): ê³ ê° ID
   - order_date (DATE): ì£¼ë¬¸ ë‚ ì§œ
   - delivery_date (DATE): ë°°ì†¡ ë‚ ì§œ
   - status (STRING): ì£¼ë¬¸ ìƒíƒœ (pending, shipped, delivered, cancelled)
   - total_amount (FLOAT64): ì£¼ë¬¸ ê¸ˆì•¡

2. products í…Œì´ë¸”:
   - product_id (STRING): ìƒí’ˆ ID
   - name (STRING): ìƒí’ˆëª…
   - category (STRING): ì¹´í…Œê³ ë¦¬
   - price (FLOAT64): ê°€ê²©

3. order_items í…Œì´ë¸”:
   - order_id (STRING): ì£¼ë¬¸ ID
   - product_id (STRING): ìƒí’ˆ ID
   - quantity (INTEGER): ìˆ˜ëŸ‰
   - unit_price (FLOAT64): ë‹¨ê°€
"""

    def natural_language_to_sql(self, question: str) -> str:
        """
        ìì—°ì–´ ì§ˆë¬¸ì„ SQLë¡œ ë³€í™˜

        Few-Shot Learningìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ
        """
        from vertexai.language_models import TextGenerationModel

        model = TextGenerationModel.from_pretrained("text-bison@002")

        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ìì—°ì–´ ì§ˆë¬¸ì„ BigQuery SQLë¡œ ë³€í™˜í•˜ì„¸ìš”.

{self.schema_context}

**ë³€í™˜ ì˜ˆì‹œ:**

ì§ˆë¬¸: "ì§€ë‚œë‹¬ ì´ ë§¤ì¶œì€?"
SQL:
```sql
SELECT SUM(total_amount) as total_revenue
FROM `{self.project_id}.analytics.orders`
WHERE order_date >= DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)
  AND order_date < DATE_TRUNC(CURRENT_DATE(), MONTH)
```

ì§ˆë¬¸: "ê°€ì¥ ë§ì´ íŒ”ë¦° ìƒí’ˆ Top 5ëŠ”?"
SQL:
```sql
SELECT p.name, SUM(oi.quantity) as total_sold
FROM `{self.project_id}.analytics.order_items` oi
JOIN `{self.project_id}.analytics.products` p ON oi.product_id = p.product_id
GROUP BY p.name
ORDER BY total_sold DESC
LIMIT 5
```

ì§ˆë¬¸: "ë°°ì†¡ ì§€ì—°ëœ ì£¼ë¬¸ì€ ëª‡ ê±´?"
SQL:
```sql
SELECT COUNT(*) as delayed_orders
FROM `{self.project_id}.analytics.orders`
WHERE status = 'shipped'
  AND DATE_DIFF(CURRENT_DATE(), order_date, DAY) > 7
```

**ì´ì œ ë‹¤ìŒ ì§ˆë¬¸ì„ SQLë¡œ ë³€í™˜í•˜ì„¸ìš”:**
ì§ˆë¬¸: "{question}"

SQL:
"""

        response = model.predict(
            prompt,
            temperature=0.2,  # ê²°ì •ì  ì¶œë ¥ (ë‚®ì€ ì°½ì˜ì„±)
            max_output_tokens=512,
            top_p=0.8,
            top_k=40
        )

        # SQL ì¶”ì¶œ (```sql ... ``` ì œê±°)
        sql = response.text.strip()
        if "```sql" in sql:
            sql = sql.split("```sql")[1].split("```")[0].strip()

        return sql

    def validate_sql(self, sql: str) -> tuple[bool, str]:
        """
        SQL ì•ˆì „ì„± ê²€ì¦

        ë³´ì•ˆ í•µì‹¬: ìœ„í—˜í•œ ì¿¼ë¦¬ ì°¨ë‹¨
        """
        sql_upper = sql.upper()

        # 1. ì½ê¸° ì „ìš© ê²€ì¦
        write_operations = ["INSERT", "UPDATE", "DELETE", "DROP", "CREATE", "ALTER"]
        for op in write_operations:
            if op in sql_upper:
                return False, f"ì“°ê¸° ì‘ì—… ê¸ˆì§€: {op}"

        # 2. í”„ë¡œì íŠ¸ ID ê²€ì¦ (ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ì ‘ê·¼ ë°©ì§€)
        if "`" in sql and self.project_id not in sql:
            return False, "ìŠ¹ì¸ë˜ì§€ ì•Šì€ í”„ë¡œì íŠ¸ ì ‘ê·¼ ì‹œë„"

        # 3. ë³µì¡ë„ ê²€ì¦ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if sql.count("JOIN") > 5:
            return False, "JOINì´ ë„ˆë¬´ ë§ìŒ (ìµœëŒ€ 5ê°œ)"

        return True, "OK"

    def execute_query(self, sql: str) -> Dict[str, Any]:
        """
        SQL ì‹¤í–‰ ë° ê²°ê³¼ ë°˜í™˜
        """
        try:
            # íƒ€ì„ì•„ì›ƒ ì„¤ì • (30ì´ˆ)
            job_config = bigquery.QueryJobConfig()
            job_config.use_query_cache = True
            job_config.maximum_bytes_billed = 10 * 1024 * 1024 * 1024  # 10GB ì œí•œ

            query_job = self.bq_client.query(sql, job_config=job_config)
            results = query_job.result(timeout=30)

            # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            rows = [dict(row) for row in results]

            return {
                "status": "success",
                "rows": rows,
                "total_rows": len(rows),
                "bytes_processed": query_job.total_bytes_processed
            }

        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }

    def answer_question(self, question: str) -> Dict[str, Any]:
        """
        ìì—°ì–´ ì§ˆë¬¸ì— ëŒ€í•œ ì „ì²´ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        """
        print(f"ì§ˆë¬¸: {question}")

        # 1. ìì—°ì–´ â†’ SQL ë³€í™˜
        sql = self.natural_language_to_sql(question)
        print(f"ìƒì„±ëœ SQL:\n{sql}\n")

        # 2. SQL ê²€ì¦
        is_valid, message = self.validate_sql(sql)
        if not is_valid:
            return {
                "status": "error",
                "error": f"SQL ê²€ì¦ ì‹¤íŒ¨: {message}",
                "sql": sql
            }

        # 3. ì¿¼ë¦¬ ì‹¤í–‰
        result = self.execute_query(sql)
        result["sql"] = sql

        return result

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    system = DominaNLQuerySystem(project_id="your-project-id")

    # ì‹¤ì œ ì§ˆë¬¸ë“¤
    questions = [
        "ì´ë²ˆ ë‹¬ ì´ ë§¤ì¶œì€ ì–¼ë§ˆì¸ê°€ìš”?",
        "ì–´ì œ ë°°ì†¡ ì™„ë£Œëœ ì£¼ë¬¸ì€ ëª‡ ê±´ì¸ê°€ìš”?",
        "ê°€ì¥ ì¸ê¸° ìˆëŠ” ìƒí’ˆ ì¹´í…Œê³ ë¦¬ëŠ”?",
        "í‰ê·  ë°°ì†¡ ì†Œìš” ì‹œê°„ì€?",
    ]

    for q in questions:
        result = system.answer_question(q)

        if result["status"] == "success":
            print(f"âœ… ê²°ê³¼ ({result['total_rows']}ê±´):")
            for row in result["rows"][:3]:  # ìƒìœ„ 3ê°œë§Œ ì¶œë ¥
                print(f"   {row}")
        else:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")

        print("-" * 50)
```

#### 4.2.4 ì„±ê³¼ ë° êµí›ˆ

**ì •ëŸ‰ì  ì„±ê³¼:**
- âœ… ë°ì´í„° ì ‘ê·¼ì„±: **80% í–¥ìƒ** (ë¹„ê¸°ìˆ  íŒ€ì›ë„ ì‚¬ìš© ê°€ëŠ¥)
- âœ… ë¶„ì„ ì†ë„: 3ì¼ â†’ **ì¦‰ì‹œ** (ì‹¤ì‹œê°„)
- âœ… ë°°ì†¡ íš¨ìœ¨: **15% ê°œì„ ** (ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ)
- âœ… SQL ì „ë¬¸ê°€ ì˜ì¡´ë„: 90% â†’ **10%**

**í•µì‹¬ ì„±ê³µ ìš”ì¸:**
1. **ë„ë©”ì¸ ì§€ì‹ ì£¼ì…**: ìŠ¤í‚¤ë§ˆ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
2. **Few-Shot Learning**: ì˜ˆì‹œ ì¿¼ë¦¬ë¡œ ì •í™•ë„ í–¥ìƒ
3. **ì•ˆì „ì„± ê²€ì¦**: ì½ê¸° ì „ìš©, ë¦¬ì†ŒìŠ¤ ì œí•œ
4. **ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤**: Streamlitìœ¼ë¡œ ì ‘ê·¼ì„± ê·¹ëŒ€í™”

**êµí›ˆ:**
- ğŸ’¡ ê¸°ìˆ ì  ì¥ë²½ ì œê±°ê°€ **ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸**ë¡œ ì§ê²°
- ğŸ’¡ ë³´ì•ˆ ê²€ì¦ì´ í•„ìˆ˜ (í”„ë¡œë•ì…˜ í™˜ê²½)
- ğŸ’¡ ìºì‹±ìœ¼ë¡œ ë¹„ìš© ì ˆê° (ë™ì¼ ì§ˆë¬¸ ë°˜ë³µ ì‹œ)

---

### 4.3 ì‚¬ë¡€ 3: Croud - ê°œë°œ ìƒì‚°ì„± 4-5ë°° í–¥ìƒ ğŸš€

**ê¸°ì—…:** Croud (ì˜êµ­ ë””ì§€í„¸ ë§ˆì¼€íŒ… ì—ì´ì „ì‹œ)
**ë„ì „ ê³¼ì œ:** ê°œë°œì ìƒì‚°ì„± í–¥ìƒ, ì½”ë“œ í’ˆì§ˆ ê°œì„ 
**ì†”ë£¨ì…˜:** Claude Sonnet + Custom Gems (ë§ì¶¤í˜• AI ì–´ì‹œìŠ¤í„´íŠ¸)

#### 4.3.1 ë¬¸ì œ ì •ì˜

**Before:**
- ë°˜ë³µì ì¸ ì½”ë“œ ì‘ì„± (CRUD, API ì—”ë“œí¬ì¸íŠ¸)
- ì½”ë“œ ë¦¬ë·°ì— ë§ì€ ì‹œê°„ ì†Œìš”
- ì‹ ê·œ ê°œë°œì ì˜¨ë³´ë”© ê¸°ê°„ ê¸¸ìŒ (3ê°œì›”)

**After:**
- AIê°€ boilerplate ì½”ë“œ ìë™ ìƒì„±
- ì‹¤ì‹œê°„ ì½”ë“œ ë¦¬ë·° ì œì•ˆ
- ì˜¨ë³´ë”© ê¸°ê°„ ë‹¨ì¶• (1ê°œì›”)

#### 4.3.2 Custom Gems ì „ëµ

**Custom Gemsì´ë€?**
Claudeì˜ ë§ì¶¤í˜• ì§€ì‹œì‚¬í•­ ì„¸íŠ¸. íšŒì‚¬/í”„ë¡œì íŠ¸ë³„ ê·œì¹™ì„ AIì—ê²Œ í•™ìŠµì‹œí‚´.

**Croudì˜ Custom Gems ì˜ˆì‹œ:**

```yaml
# croud-backend-gem.yaml
name: "Croud Backend Developer"
description: "Croud ë°±ì—”ë“œ ê°œë°œ í‘œì¤€ì„ ë”°ë¥´ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸"

instructions: |
  ë‹¹ì‹ ì€ Croudì˜ ì‹œë‹ˆì–´ ë°±ì—”ë“œ ê°œë°œìì…ë‹ˆë‹¤.

  **ì½”ë”© í‘œì¤€:**
  - ì–¸ì–´: Node.js (TypeScript)
  - í”„ë ˆì„ì›Œí¬: NestJS
  - ë°ì´í„°ë² ì´ìŠ¤: PostgreSQL + TypeORM
  - API: RESTful (OpenAPI 3.0 ë¬¸ì„œí™” í•„ìˆ˜)
  - í…ŒìŠ¤íŠ¸: Jest (ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ)

  **ì•„í‚¤í…ì²˜ íŒ¨í„´:**
  - Controller â†’ Service â†’ Repository ê³„ì¸µ ë¶„ë¦¬
  - DTO (Data Transfer Object) ì‚¬ìš©
  - ì—ëŸ¬ ì²˜ë¦¬: Custom Exception Filters
  - ë¡œê¹…: Winston (JSON í˜•ì‹)

  **ì½”ë“œ ìŠ¤íƒ€ì¼:**
  - ESLint + Prettier ì¤€ìˆ˜
  - ë³€ìˆ˜ëª…: camelCase
  - í´ë˜ìŠ¤ëª…: PascalCase
  - íŒŒì¼ëª…: kebab-case

  **ë³´ì•ˆ:**
  - ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ì— ì¸ì¦/ì¸ê°€ ì ìš©
  - ì…ë ¥ ê°’ ê²€ì¦ (class-validator)
  - SQL Injection ë°©ì§€ (Parameterized queries)
  - Rate Limiting ì ìš©

  **ë¬¸ì„œí™”:**
  - ëª¨ë“  public ë©”ì„œë“œì— JSDoc ì£¼ì„
  - READMEì— API ì‚¬ìš© ì˜ˆì‹œ í¬í•¨
  - í™˜ê²½ ë³€ìˆ˜ëŠ” .env.exampleì— ë¬¸ì„œí™”

knowledge_base:
  - file: "docs/architecture.md"
  - file: "docs/coding-standards.md"
  - file: "examples/user-service.ts"
```

#### 4.3.3 ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ

**ì‹œë‚˜ë¦¬ì˜¤ 1: CRUD API ìƒì„±**

```typescript
// ê°œë°œìê°€ AIì—ê²Œ ìš”ì²­:
// "User ì—”í‹°í‹°ì— ëŒ€í•œ CRUD APIë¥¼ ìƒì„±í•˜ì„¸ìš”.
//  í•„ë“œ: id, email, name, createdAt, updatedAt"

// AIê°€ ìƒì„±í•œ ì½”ë“œ:

// 1. Entity (user.entity.ts)
import { Entity, PrimaryGeneratedColumn, Column, CreateDateColumn, UpdateDateColumn } from 'typeorm';
import { ApiProperty } from '@nestjs/swagger';

@Entity('users')
export class User {
  @ApiProperty({ description: 'ì‚¬ìš©ì ID' })
  @PrimaryGeneratedColumn('uuid')
  id: string;

  @ApiProperty({ description: 'ì´ë©”ì¼', example: 'user@example.com' })
  @Column({ unique: true })
  email: string;

  @ApiProperty({ description: 'ì´ë¦„', example: 'í™ê¸¸ë™' })
  @Column()
  name: string;

  @ApiProperty({ description: 'ìƒì„±ì¼ì‹œ' })
  @CreateDateColumn()
  createdAt: Date;

  @ApiProperty({ description: 'ìˆ˜ì •ì¼ì‹œ' })
  @UpdateDateColumn()
  updatedAt: Date;
}

// 2. DTO (create-user.dto.ts)
import { IsEmail, IsNotEmpty, IsString, MinLength } from 'class-validator';
import { ApiProperty } from '@nestjs/swagger';

export class CreateUserDto {
  @ApiProperty({ description: 'ì´ë©”ì¼', example: 'user@example.com' })
  @IsEmail({}, { message: 'ìœ íš¨í•œ ì´ë©”ì¼ ì£¼ì†Œë¥¼ ì…ë ¥í•˜ì„¸ìš”' })
  @IsNotEmpty({ message: 'ì´ë©”ì¼ì€ í•„ìˆ˜ì…ë‹ˆë‹¤' })
  email: string;

  @ApiProperty({ description: 'ì´ë¦„', example: 'í™ê¸¸ë™' })
  @IsString()
  @IsNotEmpty({ message: 'ì´ë¦„ì€ í•„ìˆ˜ì…ë‹ˆë‹¤' })
  @MinLength(2, { message: 'ì´ë¦„ì€ ìµœì†Œ 2ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤' })
  name: string;
}

// 3. Service (user.service.ts)
import { Injectable, NotFoundException } from '@nestjs/common';
import { InjectRepository } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import { User } from './entities/user.entity';
import { CreateUserDto } from './dto/create-user.dto';
import { UpdateUserDto } from './dto/update-user.dto';
import { Logger } from '@nestjs/common';

@Injectable()
export class UserService {
  private readonly logger = new Logger(UserService.name);

  constructor(
    @InjectRepository(User)
    private userRepository: Repository<User>,
  ) {}

  /**
   * ì‚¬ìš©ì ìƒì„±
   * @param createUserDto ìƒì„±í•  ì‚¬ìš©ì ì •ë³´
   * @returns ìƒì„±ëœ ì‚¬ìš©ì
   */
  async create(createUserDto: CreateUserDto): Promise<User> {
    this.logger.log(`Creating user with email: ${createUserDto.email}`);

    const user = this.userRepository.create(createUserDto);
    const savedUser = await this.userRepository.save(user);

    this.logger.log(`User created successfully: ${savedUser.id}`);
    return savedUser;
  }

  /**
   * ëª¨ë“  ì‚¬ìš©ì ì¡°íšŒ
   * @returns ì‚¬ìš©ì ëª©ë¡
   */
  async findAll(): Promise<User[]> {
    return this.userRepository.find({
      order: { createdAt: 'DESC' },
    });
  }

  /**
   * íŠ¹ì • ì‚¬ìš©ì ì¡°íšŒ
   * @param id ì‚¬ìš©ì ID
   * @returns ì‚¬ìš©ì ì •ë³´
   * @throws NotFoundException ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
   */
  async findOne(id: string): Promise<User> {
    const user = await this.userRepository.findOne({ where: { id } });

    if (!user) {
      throw new NotFoundException(`User with ID ${id} not found`);
    }

    return user;
  }

  /**
   * ì‚¬ìš©ì ì •ë³´ ìˆ˜ì •
   * @param id ì‚¬ìš©ì ID
   * @param updateUserDto ìˆ˜ì •í•  ì •ë³´
   * @returns ìˆ˜ì •ëœ ì‚¬ìš©ì
   */
  async update(id: string, updateUserDto: UpdateUserDto): Promise<User> {
    const user = await this.findOne(id);

    Object.assign(user, updateUserDto);
    const updatedUser = await this.userRepository.save(user);

    this.logger.log(`User updated: ${id}`);
    return updatedUser;
  }

  /**
   * ì‚¬ìš©ì ì‚­ì œ
   * @param id ì‚¬ìš©ì ID
   */
  async remove(id: string): Promise<void> {
    const user = await this.findOne(id);
    await this.userRepository.remove(user);

    this.logger.log(`User deleted: ${id}`);
  }
}

// 4. Controller (user.controller.ts)
import {
  Controller,
  Get,
  Post,
  Body,
  Patch,
  Param,
  Delete,
  HttpCode,
  HttpStatus,
  UseGuards,
} from '@nestjs/common';
import {
  ApiTags,
  ApiOperation,
  ApiResponse,
  ApiBearerAuth,
} from '@nestjs/swagger';
import { UserService } from './user.service';
import { CreateUserDto } from './dto/create-user.dto';
import { UpdateUserDto } from './dto/update-user.dto';
import { JwtAuthGuard } from '../auth/guards/jwt-auth.guard';

@ApiTags('users')
@ApiBearerAuth()
@UseGuards(JwtAuthGuard)
@Controller('users')
export class UserController {
  constructor(private readonly userService: UserService) {}

  @Post()
  @ApiOperation({ summary: 'ì‚¬ìš©ì ìƒì„±' })
  @ApiResponse({ status: 201, description: 'ì‚¬ìš©ìê°€ ì„±ê³µì ìœ¼ë¡œ ìƒì„±ë¨' })
  @ApiResponse({ status: 400, description: 'ì˜ëª»ëœ ìš”ì²­' })
  async create(@Body() createUserDto: CreateUserDto) {
    return this.userService.create(createUserDto);
  }

  @Get()
  @ApiOperation({ summary: 'ëª¨ë“  ì‚¬ìš©ì ì¡°íšŒ' })
  @ApiResponse({ status: 200, description: 'ì‚¬ìš©ì ëª©ë¡ ë°˜í™˜' })
  async findAll() {
    return this.userService.findAll();
  }

  @Get(':id')
  @ApiOperation({ summary: 'íŠ¹ì • ì‚¬ìš©ì ì¡°íšŒ' })
  @ApiResponse({ status: 200, description: 'ì‚¬ìš©ì ì •ë³´ ë°˜í™˜' })
  @ApiResponse({ status: 404, description: 'ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ' })
  async findOne(@Param('id') id: string) {
    return this.userService.findOne(id);
  }

  @Patch(':id')
  @ApiOperation({ summary: 'ì‚¬ìš©ì ì •ë³´ ìˆ˜ì •' })
  @ApiResponse({ status: 200, description: 'ì‚¬ìš©ì ì •ë³´ê°€ ìˆ˜ì •ë¨' })
  @ApiResponse({ status: 404, description: 'ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ' })
  async update(@Param('id') id: string, @Body() updateUserDto: UpdateUserDto) {
    return this.userService.update(id, updateUserDto);
  }

  @Delete(':id')
  @HttpCode(HttpStatus.NO_CONTENT)
  @ApiOperation({ summary: 'ì‚¬ìš©ì ì‚­ì œ' })
  @ApiResponse({ status: 204, description: 'ì‚¬ìš©ìê°€ ì‚­ì œë¨' })
  @ApiResponse({ status: 404, description: 'ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ' })
  async remove(@Param('id') id: string) {
    await this.userService.remove(id);
  }
}

// 5. Test (user.service.spec.ts)
import { Test, TestingModule } from '@nestjs/testing';
import { getRepositoryToken } from '@nestjs/typeorm';
import { Repository } from 'typeorm';
import { UserService } from './user.service';
import { User } from './entities/user.entity';
import { NotFoundException } from '@nestjs/common';

describe('UserService', () => {
  let service: UserService;
  let repository: Repository<User>;

  const mockUserRepository = {
    create: jest.fn(),
    save: jest.fn(),
    find: jest.fn(),
    findOne: jest.fn(),
    remove: jest.fn(),
  };

  beforeEach(async () => {
    const module: TestingModule = await Test.createTestingModule({
      providers: [
        UserService,
        {
          provide: getRepositoryToken(User),
          useValue: mockUserRepository,
        },
      ],
    }).compile();

    service = module.get<UserService>(UserService);
    repository = module.get<Repository<User>>(getRepositoryToken(User));
  });

  it('should be defined', () => {
    expect(service).toBeDefined();
  });

  describe('create', () => {
    it('ì‚¬ìš©ìë¥¼ ìƒì„±í•´ì•¼ í•¨', async () => {
      const createUserDto = { email: 'test@example.com', name: 'í…ŒìŠ¤í„°' };
      const savedUser = { id: '1', ...createUserDto, createdAt: new Date(), updatedAt: new Date() };

      mockUserRepository.create.mockReturnValue(savedUser);
      mockUserRepository.save.mockResolvedValue(savedUser);

      const result = await service.create(createUserDto);

      expect(result).toEqual(savedUser);
      expect(mockUserRepository.create).toHaveBeenCalledWith(createUserDto);
      expect(mockUserRepository.save).toHaveBeenCalledWith(savedUser);
    });
  });

  describe('findOne', () => {
    it('ì‚¬ìš©ìë¥¼ ì°¾ìœ¼ë©´ ë°˜í™˜í•´ì•¼ í•¨', async () => {
      const user = { id: '1', email: 'test@example.com', name: 'í…ŒìŠ¤í„°' };
      mockUserRepository.findOne.mockResolvedValue(user);

      const result = await service.findOne('1');

      expect(result).toEqual(user);
    });

    it('ì‚¬ìš©ìë¥¼ ì°¾ì§€ ëª»í•˜ë©´ NotFoundExceptionì„ ë˜ì ¸ì•¼ í•¨', async () => {
      mockUserRepository.findOne.mockResolvedValue(null);

      await expect(service.findOne('999')).rejects.toThrow(NotFoundException);
    });
  });
});
```

**ê°œë°œ ì‹œê°„ ë¹„êµ:**
- **ìˆ˜ë™ ì‘ì„±:** 3-4ì‹œê°„
- **AI ìƒì„±:** 5-10ë¶„
- **ê°œë°œì ì‘ì—…:** ì½”ë“œ ë¦¬ë·° ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ì¶”ê°€ (30ë¶„)
- **ì´ ì‹œê°„ ì ˆê°:** **75%**

#### 4.3.4 ì„±ê³¼ ë° êµí›ˆ

**ì •ëŸ‰ì  ì„±ê³¼:**
- âœ… ìƒì‚°ì„±: **4-5ë°° í–¥ìƒ**
- âœ… ì½”ë“œ í’ˆì§ˆ: ë²„ê·¸ ê°ì†Œ 30%
- âœ… ì˜¨ë³´ë”© ê¸°ê°„: 3ê°œì›” â†’ 1ê°œì›”
- âœ… ì½”ë“œ ë¦¬ë·° ì‹œê°„: 50% ê°ì†Œ

**í•µì‹¬ ì„±ê³µ ìš”ì¸:**
1. **Custom Gems**: íšŒì‚¬ í‘œì¤€ì„ AIì—ê²Œ í•™ìŠµ
2. **ì¼ê´€ì„±**: ëª¨ë“  ê°œë°œìê°€ ë™ì¼í•œ íŒ¨í„´ ì‚¬ìš©
3. **í…ŒìŠ¤íŠ¸ í¬í•¨**: AIê°€ í…ŒìŠ¤íŠ¸ ì½”ë“œê¹Œì§€ ìƒì„±
4. **ì ì§„ì  ë„ì…**: ê°„ë‹¨í•œ ì‘ì—…ë¶€í„° ì‹œì‘ â†’ ë³µì¡í•œ ì‘ì—…ìœ¼ë¡œ í™•ëŒ€

**êµí›ˆ:**
- ğŸ’¡ AIëŠ” boilerplate ì œê±°ì— íƒì›”
- ğŸ’¡ íŒ€ í‘œì¤€ì„ AIì—ê²Œ ì£¼ì…í•˜ëŠ” ê²ƒì´ í•µì‹¬
- ğŸ’¡ ê°œë°œìëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ì§‘ì¤‘

---

### 4.4 ì‚¬ë¡€ 4: ChatPRD - PRD ì‘ì„± ì‹œê°„ 75% ì ˆê° ğŸ“

**ê¸°ì—…:** ì—¬ëŸ¬ ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ì‚¬ìš©
**ë„ì „ ê³¼ì œ:** PRD ì‘ì„±ì— ë§ì€ ì‹œê°„ ì†Œìš” (3-5ì¼)
**ì†”ë£¨ì…˜:** AI ê¸°ë°˜ PRD ìë™ ìƒì„± ë„êµ¬

#### 4.4.1 ë¬¸ì œ ì •ì˜

**PRD ì‘ì„±ì˜ ì–´ë ¤ì›€:**
- êµ¬ì¡°í™”ëœ ë¬¸ì„œ ì‘ì„± ëŠ¥ë ¥ í•„ìš”
- ê¸°ìˆ ì  ìš”êµ¬ì‚¬í•­ê³¼ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ê· í˜•
- ì´í•´ê´€ê³„ìë³„ ê´€ì  ê³ ë ¤
- ì¼ê´€ì„± ìœ ì§€ ì–´ë ¤ì›€

#### 4.4.2 ChatPRD ì†”ë£¨ì…˜

**í•µì‹¬ ê¸°ëŠ¥:**
1. ê°„ë‹¨í•œ ì•„ì´ë””ì–´ ì…ë ¥ â†’ êµ¬ì¡°í™”ëœ PRD ìƒì„±
2. ì´í•´ê´€ê³„ìë³„ ì„¹ì…˜ ìë™ ìƒì„±
3. ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìë™ ì¶”ì¶œ
4. ì„±ê³µ ì§€í‘œ ì œì•ˆ

#### 4.4.3 êµ¬í˜„ ì˜ˆì‹œ

```python
# chatprd.py
from openai import OpenAI
from typing import Dict, List
import json

class ChatPRD:
    """
    AI ê¸°ë°˜ PRD ìƒì„±ê¸°

    75% ì‹œê°„ ì ˆê°ì˜ ë¹„ê²°: í…œí”Œë¦¿ + AI ìƒì„±
    """

    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)

    def generate_prd(
        self,
        feature_idea: str,
        target_users: str,
        business_goal: str,
        tech_stack: str = ""
    ) -> str:
        """
        PRD ìƒì„±

        Args:
            feature_idea: ê¸°ëŠ¥ ì•„ì´ë””ì–´ (ê°„ë‹¨í•œ ì„¤ëª…)
            target_users: ëŒ€ìƒ ì‚¬ìš©ì
            business_goal: ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ
            tech_stack: ê¸°ìˆ  ìŠ¤íƒ (ì„ íƒ)

        Returns:
            ì™„ì „í•œ PRD ë¬¸ì„œ (Markdown)
        """

        prompt = f"""
ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ì ì¸ PRD (Product Requirement Document)ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì…ë ¥ ì •ë³´:**
- ê¸°ëŠ¥ ì•„ì´ë””ì–´: {feature_idea}
- ëŒ€ìƒ ì‚¬ìš©ì: {target_users}
- ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ: {business_goal}
- ê¸°ìˆ  ìŠ¤íƒ: {tech_stack or "ë¯¸ì •"}

**PRD êµ¬ì¡° (ë°˜ë“œì‹œ ì´ ìˆœì„œë¡œ):**

# [ê¸°ëŠ¥ëª…]

## 1. Executive Summary
- ê¸°ëŠ¥ ê°œìš” (2-3ë¬¸ì¥)
- í•µì‹¬ ê°€ì¹˜ ì œì•ˆ
- ì˜ˆìƒ ì„íŒ©íŠ¸

## 2. Background & Problem Statement
- í˜„ì¬ ë¬¸ì œ/ê¸°íšŒ
- ì‚¬ìš©ì í˜ì¸ í¬ì¸íŠ¸
- ì‹œì¥ ìƒí™©

## 3. Goals & Success Metrics
- ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ (ì •ëŸ‰ì )
- ì‚¬ìš©ì ëª©í‘œ
- ì„±ê³µ ì§€í‘œ (KPI)

## 4. User Stories
ì ì–´ë„ 5ê°œì˜ ì‚¬ìš©ì ìŠ¤í† ë¦¬:
- As a [ì—­í• ], I want [ê¸°ëŠ¥], so that [ì´ìœ ]

## 5. Functional Requirements
### 5.1 Core Features (Must-have)
- í•„ìˆ˜ ê¸°ëŠ¥ ëª©ë¡

### 5.2 Nice-to-have Features
- ë¶€ê°€ ê¸°ëŠ¥ ëª©ë¡

## 6. Non-Functional Requirements
- ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­
- ë³´ì•ˆ ìš”êµ¬ì‚¬í•­
- í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

## 7. User Experience
- ì£¼ìš” ì‚¬ìš©ì í”Œë¡œìš°
- UI/UX ê³ ë ¤ì‚¬í•­
- ì ‘ê·¼ì„± ìš”êµ¬ì‚¬í•­

## 8. Technical Considerations
- ê¸°ìˆ  ìŠ¤íƒ ë° ì•„í‚¤í…ì²˜
- ë°ì´í„° ëª¨ë¸
- í†µí•© ì§€ì  (Third-party APIs ë“±)

## 9. Timeline & Milestones
- Phase 1: MVP (ì£¼ìš” ê¸°ëŠ¥)
- Phase 2: ì¶”ê°€ ê¸°ëŠ¥
- Phase 3: ìµœì í™”

## 10. Risks & Mitigation
- ì˜ˆìƒ ë¦¬ìŠ¤í¬
- ì™„í™” ì „ëµ

## 11. Open Questions
- ê²°ì •ì´ í•„ìš”í•œ ì‚¬í•­ë“¤

**ì‘ì„± ê·œì¹™:**
- êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ í‘œí˜„ ì‚¬ìš©
- ê¸°ìˆ  ìš©ì–´ëŠ” ì„¤ëª… í¬í•¨
- ê° ì„¹ì…˜ì€ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ
- ê°œë°œíŒ€ì´ ë°”ë¡œ ì°©ìˆ˜í•  ìˆ˜ ìˆë„ë¡ ìƒì„¸í•˜ê²Œ

PRDë¥¼ ì‘ì„±í•˜ì„¸ìš”:
"""

        response = self.client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ì „ë¬¸ í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤. ëª…í™•í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ PRDë¥¼ ì‘ì„±í•©ë‹ˆë‹¤."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.7,
            max_tokens=3000
        )

        prd = response.choices[0].message.content

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        metadata = f"""
---
**ë¬¸ì„œ ì •ë³´:**
- ìƒì„± ì¼ì: {datetime.now().strftime("%Y-%m-%d")}
- ìƒì„± ë„êµ¬: ChatPRD (AI-powered)
- ë²„ì „: 1.0 (ì´ˆì•ˆ)

âš ï¸ **ì£¼ì˜:** ì´ ë¬¸ì„œëŠ” AIê°€ ìƒì„±í•œ ì´ˆì•ˆì…ë‹ˆë‹¤.
íŒ€ ë¦¬ë·° ë° ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
---

"""

        return metadata + prd

    def refine_section(self, prd: str, section: str, feedback: str) -> str:
        """
        íŠ¹ì • ì„¹ì…˜ ê°œì„ 

        ì‚¬ìš©ì í”¼ë“œë°±ì„ ë°˜ì˜í•œ ë°˜ë³µ ê°œì„ 
        """

        prompt = f"""
ë‹¤ìŒ PRDì˜ '{section}' ì„¹ì…˜ì„ ê°œì„ í•˜ì„¸ìš”.

**í˜„ì¬ PRD:**
{prd}

**ê°œì„  ìš”ì²­:**
{feedback}

ê°œì„ ëœ '{section}' ì„¹ì…˜ë§Œ ì¶œë ¥í•˜ì„¸ìš” (ë‹¤ë¥¸ ë¶€ë¶„ì€ ê·¸ëŒ€ë¡œ):
"""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )

        return response.choices[0].message.content

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    from datetime import datetime

    chatprd = ChatPRD(api_key="YOUR_API_KEY")

    # PRD ìƒì„±
    prd = chatprd.generate_prd(
        feature_idea="ì‚¬ìš©ìê°€ ì•Œë¦¼ ìˆ˜ì‹  ë°©ì‹ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥",
        target_users="ëª¨ë°”ì¼ ì•± ì‚¬ìš©ì (25-45ì„¸, ì§ì¥ì¸)",
        business_goal="ì‚¬ìš©ì ì°¸ì—¬ë„ 20% í–¥ìƒ, ì•Œë¦¼ í•´ì œìœ¨ 50% ê°ì†Œ",
        tech_stack="React Native, Firebase Cloud Messaging, Node.js"
    )

    # íŒŒì¼ë¡œ ì €ì¥
    with open("PRD_Notification_Settings.md", "w", encoding="utf-8") as f:
        f.write(prd)

    print("âœ… PRD ìƒì„± ì™„ë£Œ: PRD_Notification_Settings.md")
```

#### 4.4.4 ì„±ê³¼ ë° êµí›ˆ

**ì •ëŸ‰ì  ì„±ê³¼:**
- âœ… ì‘ì„± ì‹œê°„: 3-5ì¼ â†’ **6ì‹œê°„** (75% ì ˆê°)
  - AI ìƒì„±: 10ë¶„
  - ë¦¬ë·° ë° ìˆ˜ì •: 2ì‹œê°„
  - íŒ€ í”¼ë“œë°± ë°˜ì˜: 3ì‹œê°„
  - ìµœì¢… ê²€í† : 1ì‹œê°„
- âœ… ë¬¸ì„œ í’ˆì§ˆ: ì¼ê´€ì„± í–¥ìƒ
- âœ… ëˆ„ë½ í•­ëª©: 90% ê°ì†Œ (êµ¬ì¡°í™”ëœ í…œí”Œë¦¿)

**í•µì‹¬ ì„±ê³µ ìš”ì¸:**
1. **êµ¬ì¡°í™”ëœ í”„ë¡¬í”„íŠ¸**: ëª…í™•í•œ ì„¹ì…˜ ì •ì˜
2. **ë„ë©”ì¸ ì§€ì‹**: PM ì—­í•  ë¶€ì—¬
3. **ë°˜ë³µ ê°œì„ **: ì„¹ì…˜ë³„ ìˆ˜ì • ê¸°ëŠ¥
4. **ì‚¬ëŒì˜ íŒë‹¨**: AIëŠ” ì´ˆì•ˆ, ìµœì¢… ê²°ì •ì€ PM

**êµí›ˆ:**
- ğŸ’¡ AIëŠ” "ì´ˆì•ˆ ì‘ì„±"ì— ìµœì 
- ğŸ’¡ êµ¬ì¡°í™”ëœ ì¶œë ¥ì´ í’ˆì§ˆ í–¥ìƒì˜ í•µì‹¬
- ğŸ’¡ ë°˜ë³µ ê°œì„  í”„ë¡œì„¸ìŠ¤ í•„ìˆ˜

---

### 4.5 ì„±ê³µ ì‚¬ë¡€ ê³µí†µ íŒ¨í„´ ë¶„ì„ ğŸ”

ëª¨ë“  ì„±ê³µ ì‚¬ë¡€ì—ì„œ ë°œê²¬ëœ **ê³µí†µ íŒ¨í„´**:

#### 4.5.1 ë¬¸ì œ ì„ ì •

**ì„±ê³µí•˜ëŠ” AI ë„êµ¬:**
- âœ… ë°˜ë³µì ì´ê³  êµ¬ì¡°í™”ëœ ì‘ì—…
- âœ… ëª…í™•í•œ ì…ì¶œë ¥ ì •ì˜
- âœ… ì¸¡ì • ê°€ëŠ¥í•œ ì„±ê³¼

**ì‹¤íŒ¨í•˜ëŠ” AI ë„êµ¬:**
- âŒ ì°½ì˜ì„±ì´ í•µì‹¬ì¸ ì‘ì—…
- âŒ ë¶ˆëª…í™•í•œ ìš”êµ¬ì‚¬í•­
- âŒ ì£¼ê´€ì  íŒë‹¨ì´ í•„ìš”í•œ ì‘ì—…

#### 4.5.2 í”„ë¡¬í”„íŠ¸ ì„¤ê³„

**íš¨ê³¼ì ì¸ í”„ë¡¬í”„íŠ¸:**
```python
í”„ë¡¬í”„íŠ¸ = (
    ì—­í•  ì •ì˜ +
    ë„ë©”ì¸ ì§€ì‹ (ìŠ¤í‚¤ë§ˆ, í‘œì¤€ ë“±) +
    ëª…í™•í•œ ì¶œë ¥ í˜•ì‹ +
    Few-Shot ì˜ˆì‹œ +
    ì œì•½ì¡°ê±´
)
```

#### 4.5.3 ì•ˆì „ì¥ì¹˜

**í•„ìˆ˜ ì•ˆì „ì¥ì¹˜:**
1. **ê²€ì¦ ë¡œì§**: AI ì¶œë ¥ì„ ë£° ê¸°ë°˜ìœ¼ë¡œ ê²€ì¦
2. **ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜**: ì‹¤íŒ¨ ì‹œ í”¼ë“œë°±ê³¼ í•¨ê»˜ ì¬ì‹œë„
3. **ì‚¬ëŒì˜ ìµœì¢… ìŠ¹ì¸**: ì¤‘ìš”í•œ ê²°ì •ì€ ì‚¬ëŒì´ í™•ì¸
4. **ëª¨ë‹ˆí„°ë§**: ì„±ëŠ¥ ì§€í‘œ ì§€ì† ì¶”ì 

#### 4.5.4 ROI ê³„ì‚°

**íˆ¬ì ëŒ€ë¹„ íš¨ê³¼ ì¸¡ì •:**

| í•­ëª© | Before | After | ê°œì„ ìœ¨ |
|------|--------|-------|--------|
| **Gazelle** | 4ì‹œê°„/ê±´ | 10ì´ˆ/ê±´ | 99.9% |
| **Domina** | 3ì¼ | ì¦‰ì‹œ | 100% |
| **Croud** | 4ì‹œê°„ | 1ì‹œê°„ | 75% |
| **ChatPRD** | 5ì¼ | 6ì‹œê°„ | 75% |

**ë¹„ìš© ì ˆê°:**
```
ì›” ë¹„ìš© ì ˆê° = (ì ˆê° ì‹œê°„ Ã— ì‹œê°„ë‹¹ ì¸ê±´ë¹„) - AI API ë¹„ìš©

ì˜ˆì‹œ (ChatPRD):
- ì ˆê° ì‹œê°„: 4ì¼ Ã— 20 PRD/ì›” = 80ì¼
- ì¸ê±´ë¹„: $50/ì‹œê°„ Ã— 8ì‹œê°„ Ã— 80ì¼ = $32,000
- AI ë¹„ìš©: $100/ì›”
- ìˆœ ì ˆê°: $31,900/ì›”
```

---

**âœ… Phase 2 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] 4ê°€ì§€ ì‹¤ì œ ì‚¬ë¡€ ë¶„ì„ ì™„ë£Œ
- [ ] ì„±ê³µ ìš”ì¸ ì´í•´
- [ ] ê³µí†µ íŒ¨í„´ íŒŒì•…
- [ ] ROI ê³„ì‚° ë°©ë²• í•™ìŠµ

**ë‹¤ìŒ ë‹¨ê³„:** Phase 3ì—ì„œ ê¸°íšììš© ë„êµ¬ë¥¼ ì§ì ‘ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

## 5. Phase 3: ê¸°íšììš© ë„êµ¬ êµ¬ì¶•

ì´ ì„¹ì…˜ì—ì„œëŠ” ê¸°íšìê°€ ì‹¤ì œë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” 3ê°€ì§€ í•µì‹¬ ë„êµ¬ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

### 5.1 ë„êµ¬ 1: PRD ìë™ ìƒì„±ê¸° ğŸ“

#### 5.1.1 ëª©í‘œ ë° ê¸°ëŒ€ íš¨ê³¼

**ëª©í‘œ:**
- ê°„ë‹¨í•œ ì•„ì´ë””ì–´ ì…ë ¥ â†’ ì™„ì „í•œ PRD ë¬¸ì„œ ìƒì„±
- ì‘ì„± ì‹œê°„ 75% ì ˆê° (5ì¼ â†’ 6ì‹œê°„)

**ê¸°ëŒ€ íš¨ê³¼:**
- ì¼ê´€ëœ ë¬¸ì„œ êµ¬ì¡°
- ëˆ„ë½ í•­ëª© ìµœì†Œí™”
- íŒ€ì› ê°„ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ê°œì„ 

#### 5.1.2 êµ¬í˜„ (ì™„ì „í•œ ì½”ë“œ)

```python
# prd_generator.py
"""
PRD ìë™ ìƒì„±ê¸°

ì‚¬ìš©ë²•:
    python prd_generator.py --idea "ì•Œë¦¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê¸°ëŠ¥" \\
                           --users "25-45ì„¸ ì§ì¥ì¸" \\
                           --goal "ì°¸ì—¬ë„ 20% í–¥ìƒ"
"""

from openai import OpenAI
from typing import Dict, Optional
import argparse
import json
from datetime import datetime
from pathlib import Path

class PRDGenerator:
    """PRD ìë™ ìƒì„±ê¸°"""

    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)
        self.template = self._load_template()

    def _load_template(self) -> str:
        """PRD í…œí”Œë¦¿ ë¡œë“œ"""
        return """
# {title}

## 1. Executive Summary
{executive_summary}

## 2. Background & Problem Statement
{background}

## 3. Goals & Success Metrics
{goals}

## 4. User Stories
{user_stories}

## 5. Functional Requirements
{functional_requirements}

## 6. Non-Functional Requirements
{non_functional_requirements}

## 7. User Experience
{user_experience}

## 8. Technical Considerations
{technical_considerations}

## 9. Timeline & Milestones
{timeline}

## 10. Risks & Mitigation
{risks}

## 11. Open Questions
{open_questions}
"""

    def generate_prd(
        self,
        feature_idea: str,
        target_users: str,
        business_goal: str,
        tech_stack: str = "",
        company_context: str = ""
    ) -> Dict[str, str]:
        """
        PRD ìƒì„±

        Returns:
            ê° ì„¹ì…˜ì˜ ë‚´ìš©ì„ ë‹´ì€ ë”•ì…”ë„ˆë¦¬
        """

        system_prompt = """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ í”„ë¡œë•íŠ¸ ë§¤ë‹ˆì €ì…ë‹ˆë‹¤.
CRISP í”„ë ˆì„ì›Œí¬ë¥¼ ë”°ë¥´ê³ , ì‹¤í–‰ ê°€ëŠ¥í•œ PRDë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

PRD ì‘ì„± ì›ì¹™:
1. êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì§€í‘œ ì‚¬ìš©
2. ê°œë°œíŒ€ì´ ë°”ë¡œ êµ¬í˜„ ê°€ëŠ¥í•˜ë„ë¡ ëª…í™•í•˜ê²Œ
3. ì´í•´ê´€ê³„ì ëª¨ë‘ê°€ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ
4. ëˆ„ë½ ì—†ì´ ì™„ì „í•˜ê²Œ"""

        # ê° ì„¹ì…˜ë³„ë¡œ ìƒì„± (ë¹„ìš© ë° ì •í™•ë„ ìµœì í™”)
        sections = {}

        # 1. Executive Summary
        sections['title'], sections['executive_summary'] = self._generate_executive_summary(
            feature_idea, business_goal, system_prompt
        )

        # 2. Background
        sections['background'] = self._generate_section(
            "Background & Problem Statement",
            f"""ë‹¤ìŒ ê¸°ëŠ¥ì— ëŒ€í•œ ë°°ê²½ê³¼ ë¬¸ì œ ì •ì˜ë¥¼ ì‘ì„±í•˜ì„¸ìš”:
- ê¸°ëŠ¥: {feature_idea}
- ëŒ€ìƒ ì‚¬ìš©ì: {target_users}
- íšŒì‚¬ ë§¥ë½: {company_context or 'ìŠ¤íƒ€íŠ¸ì—…'}

í¬í•¨ ì‚¬í•­:
- í˜„ì¬ ë¬¸ì œ/ê¸°íšŒ (2-3ë¬¸ì¥)
- ì‚¬ìš©ì í˜ì¸ í¬ì¸íŠ¸ (êµ¬ì²´ì  ì˜ˆì‹œ)
- ì‹œì¥ ìƒí™© ë˜ëŠ” ê²½ìŸì‚¬ ë¶„ì„""",
            system_prompt
        )

        # 3. Goals & Success Metrics
        sections['goals'] = self._generate_section(
            "Goals & Success Metrics",
            f"""ë‹¤ìŒ ëª©í‘œì— ëŒ€í•œ ì„±ê³µ ì§€í‘œë¥¼ ì‘ì„±í•˜ì„¸ìš”:
- ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ: {business_goal}

í¬í•¨ ì‚¬í•­:
**ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ (ì •ëŸ‰ì ):**
- ì˜ˆ: ë§¤ì¶œ 20% ì¦ê°€, ì´íƒˆë¥  15% ê°ì†Œ

**ì‚¬ìš©ì ëª©í‘œ:**
- ì˜ˆ: ì‘ì—… ì‹œê°„ 50% ë‹¨ì¶•

**ì£¼ìš” ì„±ê³µ ì§€í‘œ (KPI):**
- Leading indicator: ì¶œì‹œ 1ê°œì›” ë‚´ ì¸¡ì •
- Lagging indicator: ì¶œì‹œ 3-6ê°œì›” í›„ ì¸¡ì •

ì¸¡ì • ê°€ëŠ¥í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 4. User Stories
        sections['user_stories'] = self._generate_section(
            "User Stories",
            f"""ë‹¤ìŒ ê¸°ëŠ¥ì— ëŒ€í•œ ì‚¬ìš©ì ìŠ¤í† ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”:
- ê¸°ëŠ¥: {feature_idea}
- ì‚¬ìš©ì: {target_users}

ìµœì†Œ 5ê°œ ì´ìƒ, ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ:
- As a [ì—­í• ], I want [ê¸°ëŠ¥], so that [ì´ìœ ]

ì˜ˆì‹œ:
- As a busy professional, I want to customize notification settings, so that I only receive important alerts during work hours.

ë‹¤ì–‘í•œ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬í•¨í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 5. Functional Requirements
        sections['functional_requirements'] = self._generate_section(
            "Functional Requirements",
            f"""ë‹¤ìŒ ê¸°ëŠ¥ì— ëŒ€í•œ ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš”:
- ê¸°ëŠ¥: {feature_idea}

êµ¬ì¡°:
### 5.1 Core Features (Must-have)
- [ ] ê¸°ëŠ¥ 1: ì„¤ëª…
- [ ] ê¸°ëŠ¥ 2: ì„¤ëª…
...

### 5.2 Nice-to-have Features
- [ ] ë¶€ê°€ ê¸°ëŠ¥ 1
- [ ] ë¶€ê°€ ê¸°ëŠ¥ 2

ì²´í¬ë°•ìŠ¤ í˜•ì‹ìœ¼ë¡œ, êµ¬í˜„ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 6. Non-Functional Requirements
        sections['non_functional_requirements'] = self._generate_section(
            "Non-Functional Requirements",
            f"""ë¹„ê¸°ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš”:

í¬í•¨ ì‚¬í•­:
**ì„±ëŠ¥:**
- ì‘ë‹µ ì‹œê°„: < 2ì´ˆ
- ì²˜ë¦¬ëŸ‰: 1000 req/sec

**ë³´ì•ˆ:**
- ì¸ì¦/ì¸ê°€ ë°©ì‹
- ë°ì´í„° ì•”í˜¸í™”

**í™•ì¥ì„±:**
- ì‚¬ìš©ì ì¦ê°€ ì‹œ ëŒ€ì‘ ë°©ì•ˆ

**ê°€ìš©ì„±:**
- Uptime: 99.9%

êµ¬ì²´ì  ìˆ˜ì¹˜ì™€ í•¨ê»˜ ì‘ì„±í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 7. User Experience
        sections['user_experience'] = self._generate_section(
            "User Experience",
            f"""ì‚¬ìš©ì ê²½í—˜ ì„¤ê³„ë¥¼ ì‘ì„±í•˜ì„¸ìš”:
- ê¸°ëŠ¥: {feature_idea}

í¬í•¨ ì‚¬í•­:
**ì£¼ìš” ì‚¬ìš©ì í”Œë¡œìš°:**
1. ì§„ì…ì  â†’ 2. í•µì‹¬ ì•¡ì…˜ â†’ 3. ê²°ê³¼

**UI/UX ê³ ë ¤ì‚¬í•­:**
- ì§ê´€ì„±, ì ‘ê·¼ì„±, ì¼ê´€ì„±

**ì ‘ê·¼ì„± ìš”êµ¬ì‚¬í•­:**
- ìŠ¤í¬ë¦° ë¦¬ë” ì§€ì›
- í‚¤ë³´ë“œ ë„¤ë¹„ê²Œì´ì…˜

êµ¬ì²´ì ì¸ í™”ë©´ íë¦„ì„ í¬í•¨í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 8. Technical Considerations
        sections['technical_considerations'] = self._generate_section(
            "Technical Considerations",
            f"""ê¸°ìˆ ì  ê³ ë ¤ì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš”:
- ê¸°ìˆ  ìŠ¤íƒ: {tech_stack or 'TBD'}

í¬í•¨ ì‚¬í•­:
**ì•„í‚¤í…ì²˜:**
- Frontend: ê¸°ìˆ  ë° êµ¬ì¡°
- Backend: ê¸°ìˆ  ë° êµ¬ì¡°
- Database: ì„ íƒ ë° ìŠ¤í‚¤ë§ˆ

**ë°ì´í„° ëª¨ë¸:**
- ì£¼ìš” ì—”í‹°í‹° ë° ê´€ê³„

**Third-party í†µí•©:**
- í•„ìš”í•œ ì™¸ë¶€ API/ì„œë¹„ìŠ¤

ê°œë°œíŒ€ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‘ì„±í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 9. Timeline
        sections['timeline'] = self._generate_section(
            "Timeline & Milestones",
            """ê°œë°œ íƒ€ì„ë¼ì¸ì„ ì‘ì„±í•˜ì„¸ìš”:

**Phase 1: MVP (4-6ì£¼)**
- Week 1-2: ì„¤ê³„ ë° í”„ë¡œí† íƒ€ì…
- Week 3-4: í•µì‹¬ ê¸°ëŠ¥ ê°œë°œ
- Week 5-6: í…ŒìŠ¤íŠ¸ ë° ë²„ê·¸ ìˆ˜ì •

**Phase 2: ì¶”ê°€ ê¸°ëŠ¥ (4ì£¼)**
- Nice-to-have ê¸°ëŠ¥ êµ¬í˜„

**Phase 3: ìµœì í™” (2ì£¼)**
- ì„±ëŠ¥ ê°œì„  ë° ëª¨ë‹ˆí„°ë§

í˜„ì‹¤ì ì¸ ì¼ì •ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 10. Risks
        sections['risks'] = self._generate_section(
            "Risks & Mitigation",
            f"""ë‹¤ìŒ ê¸°ëŠ¥ì˜ ë¦¬ìŠ¤í¬ë¥¼ ë¶„ì„í•˜ì„¸ìš”:
- ê¸°ëŠ¥: {feature_idea}

í˜•ì‹:
**ë¦¬ìŠ¤í¬ 1: [ì œëª©]**
- ê°€ëŠ¥ì„±: High/Medium/Low
- ì˜í–¥: High/Medium/Low
- ì™„í™” ì „ëµ: êµ¬ì²´ì  ë°©ì•ˆ

ìµœì†Œ 3ê°€ì§€ ë¦¬ìŠ¤í¬ë¥¼ ì‹ë³„í•˜ì„¸ìš”.""",
            system_prompt
        )

        # 11. Open Questions
        sections['open_questions'] = self._generate_section(
            "Open Questions",
            """ê²°ì •ì´ í•„ìš”í•œ ì‚¬í•­ë“¤ì„ ë‚˜ì—´í•˜ì„¸ìš”:

- [ ] ì§ˆë¬¸ 1: ì˜ˆ) ì´ˆê¸° ì§€ì› í”Œë«í¼ì€? (iOS only vs iOS+Android)
- [ ] ì§ˆë¬¸ 2: ì˜ˆ) ë°ì´í„° ë³´ê´€ ê¸°ê°„ì€?
- [ ] ì§ˆë¬¸ 3: ì˜ˆ) ì•Œë¦¼ ì „ì†¡ ë°©ì‹ì€? (Push vs Email vs SMS)

íŒ€ ë…¼ì˜ê°€ í•„ìš”í•œ í•­ëª©ë“¤ì„ í¬í•¨í•˜ì„¸ìš”.""",
            system_prompt
        )

        return sections

    def _generate_executive_summary(
        self,
        feature_idea: str,
        business_goal: str,
        system_prompt: str
    ) -> tuple[str, str]:
        """Executive Summary ë° ì œëª© ìƒì„±"""

        prompt = f"""ë‹¤ìŒ ê¸°ëŠ¥ì— ëŒ€í•œ ì œëª©ê³¼ Executive Summaryë¥¼ ì‘ì„±í•˜ì„¸ìš”:

**ê¸°ëŠ¥:** {feature_idea}
**ëª©í‘œ:** {business_goal}

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "title": "ê°„ê²°í•œ ê¸°ëŠ¥ëª… (3-5ë‹¨ì–´)",
  "summary": "2-3ë¬¸ì¥ìœ¼ë¡œ ê¸°ëŠ¥ ê°œìš”, í•µì‹¬ ê°€ì¹˜, ì˜ˆìƒ ì„íŒ©íŠ¸ë¥¼ ì„¤ëª…"
}}

ì˜ˆì‹œ:
{{
  "title": "ì‚¬ìš©ì ì•Œë¦¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•",
  "summary": "ì‚¬ìš©ìê°€ ì•Œë¦¼ ìˆ˜ì‹  ë°©ì‹, ì‹œê°„, ì±„ë„ì„ ììœ ë¡­ê²Œ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ì•Œë¦¼ìœ¼ë¡œ ì¸í•œ ì•± ì´íƒˆì„ ë°©ì§€í•˜ê³ , ì‚¬ìš©ì ì°¸ì—¬ë„ë¥¼ 20% í–¥ìƒì‹œí‚¬ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤. MVPëŠ” 6ì£¼ ë‚´ ì¶œì‹œ ê°€ëŠ¥í•©ë‹ˆë‹¤."
}}
"""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=300
        )

        # JSON íŒŒì‹±
        result_text = response.choices[0].message.content.strip()
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0].strip()

        result = json.loads(result_text)
        return result['title'], result['summary']

    def _generate_section(
        self,
        section_name: str,
        prompt: str,
        system_prompt: str
    ) -> str:
        """ê°œë³„ ì„¹ì…˜ ìƒì„±"""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=800
        )

        return response.choices[0].message.content.strip()

    def save_prd(self, sections: Dict[str, str], output_path: str):
        """PRDë¥¼ íŒŒì¼ë¡œ ì €ì¥"""

        # ë©”íƒ€ë°ì´í„° ì¶”ê°€
        metadata = f"""---
**ë¬¸ì„œ ì •ë³´**
- ìƒì„± ì¼ì: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
- ìƒì„± ë„êµ¬: PRD Generator (AI-powered)
- ë²„ì „: 1.0 (ì´ˆì•ˆ)

âš ï¸ **ì£¼ì˜ì‚¬í•­**
ì´ ë¬¸ì„œëŠ” AIê°€ ìƒì„±í•œ ì´ˆì•ˆì…ë‹ˆë‹¤.
ë°˜ë“œì‹œ íŒ€ ë¦¬ë·° ë° ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.
---

"""

        # í…œí”Œë¦¿ì— ì„¹ì…˜ ì‚½ì…
        prd_content = metadata + self.template.format(**sections)

        # íŒŒì¼ ì €ì¥
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(prd_content)

        print(f"âœ… PRD ìƒì„± ì™„ë£Œ: {output_path}")
        print(f"ğŸ“„ ì´ {len(prd_content)} ì")

        # í†µê³„ ì¶œë ¥
        self._print_statistics(prd_content)

    def _print_statistics(self, content: str):
        """ë¬¸ì„œ í†µê³„ ì¶œë ¥"""
        lines = content.split('\n')
        sections = [line for line in lines if line.startswith('##')]

        print(f"\nğŸ“Š ë¬¸ì„œ í†µê³„:")
        print(f"  - ì´ ë¼ì¸ ìˆ˜: {len(lines)}")
        print(f"  - ì„¹ì…˜ ìˆ˜: {len(sections)}")
        print(f"  - ë‹¨ì–´ ìˆ˜: {len(content.split())}")

def main():
    """CLI ì§„ì…ì """

    parser = argparse.ArgumentParser(
        description='AI ê¸°ë°˜ PRD ìë™ ìƒì„±ê¸°'
    )
    parser.add_argument(
        '--idea',
        required=True,
        help='ê¸°ëŠ¥ ì•„ì´ë””ì–´ (ì˜ˆ: ì‚¬ìš©ì ì•Œë¦¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•)'
    )
    parser.add_argument(
        '--users',
        required=True,
        help='ëŒ€ìƒ ì‚¬ìš©ì (ì˜ˆ: 25-45ì„¸ ì§ì¥ì¸)'
    )
    parser.add_argument(
        '--goal',
        required=True,
        help='ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ (ì˜ˆ: ì°¸ì—¬ë„ 20%% í–¥ìƒ)'
    )
    parser.add_argument(
        '--tech-stack',
        default='',
        help='ê¸°ìˆ  ìŠ¤íƒ (ì˜ˆ: React Native, Node.js)'
    )
    parser.add_argument(
        '--context',
        default='',
        help='íšŒì‚¬/í”„ë¡œì íŠ¸ ë§¥ë½'
    )
    parser.add_argument(
        '--output',
        default='PRD_{timestamp}.md',
        help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--api-key',
        help='OpenAI API í‚¤ (ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEY ì‚¬ìš©)'
    )

    args = parser.parse_args()

    # API í‚¤ ê°€ì ¸ì˜¤ê¸°
    import os
    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("   --api-key ì˜µì…˜ ë˜ëŠ” OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return

    # PRD ìƒì„±
    generator = PRDGenerator(api_key=api_key)

    print("ğŸš€ PRD ìƒì„± ì‹œì‘...")
    print(f"  ê¸°ëŠ¥: {args.idea}")
    print(f"  ëŒ€ìƒ: {args.users}")
    print(f"  ëª©í‘œ: {args.goal}")
    print()

    sections = generator.generate_prd(
        feature_idea=args.idea,
        target_users=args.users,
        business_goal=args.goal,
        tech_stack=args.tech_stack,
        company_context=args.context
    )

    # ì¶œë ¥ íŒŒì¼ëª… ìƒì„±
    if '{timestamp}' in args.output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = args.output.replace('{timestamp}', timestamp)
    else:
        output_path = args.output

    generator.save_prd(sections, output_path)

if __name__ == '__main__':
    main()
```

#### 5.1.3 ì‚¬ìš© ì˜ˆì‹œ

```bash
# ê¸°ë³¸ ì‚¬ìš©
python prd_generator.py \\
  --idea "ì‚¬ìš©ì ì•Œë¦¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê¸°ëŠ¥" \\
  --users "25-45ì„¸ ëª¨ë°”ì¼ ì•± ì‚¬ìš©ì" \\
  --goal "ì‚¬ìš©ì ì°¸ì—¬ë„ 20% í–¥ìƒ, ì•Œë¦¼ í•´ì œìœ¨ 50% ê°ì†Œ"

# ê³ ê¸‰ ì‚¬ìš© (ê¸°ìˆ  ìŠ¤íƒ í¬í•¨)
python prd_generator.py \\
  --idea "ì‹¤ì‹œê°„ í˜‘ì—… í¸ì§‘ ê¸°ëŠ¥" \\
  --users "íŒ€ ë‹¨ìœ„ ì§€ì‹ ê·¼ë¡œì" \\
  --goal "í˜‘ì—… íš¨ìœ¨ì„± 40% í–¥ìƒ" \\
  --tech-stack "React, WebSocket, Redis, PostgreSQL" \\
  --context "B2B SaaS ìŠ¤íƒ€íŠ¸ì—…, í˜„ì¬ ì‚¬ìš©ì 5000ëª…" \\
  --output "PRD_Realtime_Collaboration.md"
```

#### 5.1.4 ì‹¤í–‰ ê²°ê³¼

```
ğŸš€ PRD ìƒì„± ì‹œì‘...
  ê¸°ëŠ¥: ì‚¬ìš©ì ì•Œë¦¼ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê¸°ëŠ¥
  ëŒ€ìƒ: 25-45ì„¸ ëª¨ë°”ì¼ ì•± ì‚¬ìš©ì
  ëª©í‘œ: ì‚¬ìš©ì ì°¸ì—¬ë„ 20% í–¥ìƒ, ì•Œë¦¼ í•´ì œìœ¨ 50% ê°ì†Œ

âœ… PRD ìƒì„± ì™„ë£Œ: PRD_20250115_143022.md
ğŸ“„ ì´ 4523 ì

ğŸ“Š ë¬¸ì„œ í†µê³„:
  - ì´ ë¼ì¸ ìˆ˜: 187
  - ì„¹ì…˜ ìˆ˜: 11
  - ë‹¨ì–´ ìˆ˜: 1245
```

---

### 5.2 ë„êµ¬ 2: íšŒì˜ë¡ ìë™ ìš”ì•½ê¸° ğŸ™ï¸

#### 5.2.1 ëª©í‘œ ë° ê¸°ëŒ€ íš¨ê³¼

**ëª©í‘œ:**
- íšŒì˜ ë‚´ìš©(í…ìŠ¤íŠ¸/ìŒì„±) â†’ êµ¬ì¡°í™”ëœ íšŒì˜ë¡
- ì•¡ì…˜ ì•„ì´í…œ ìë™ ì¶”ì¶œ
- ì‘ì„± ì‹œê°„ 90% ì ˆê° (1ì‹œê°„ â†’ 5ë¶„)

**ê¸°ëŒ€ íš¨ê³¼:**
- íšŒì˜ ì°¸ì„ìëŠ” ë‚´ìš©ì— ì§‘ì¤‘
- ì¼ê´€ëœ íšŒì˜ë¡ í˜•ì‹
- ì•¡ì…˜ ì•„ì´í…œ ë†“ì¹¨ ë°©ì§€

#### 5.2.2 êµ¬í˜„ (ì™„ì „í•œ ì½”ë“œ)

```python
# meeting_summarizer.py
"""
íšŒì˜ë¡ ìë™ ìš”ì•½ê¸°

ì‚¬ìš©ë²•:
    python meeting_summarizer.py --input meeting_transcript.txt
    python meeting_summarizer.py --audio meeting_recording.mp3
"""

from openai import OpenAI
from typing import List, Dict
import argparse
from pathlib import Path
from datetime import datetime
import json

class MeetingSummarizer:
    """íšŒì˜ë¡ ìë™ ìš”ì•½ê¸°"""

    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)

    def transcribe_audio(self, audio_path: str) -> str:
        """
        ìŒì„± íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜

        Args:
            audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ (.mp3, .wav, .m4a ë“±)

        Returns:
            ë³€í™˜ëœ í…ìŠ¤íŠ¸
        """
        print(f"ğŸ™ï¸  ìŒì„± íŒŒì¼ ë³€í™˜ ì¤‘: {audio_path}")

        with open(audio_path, 'rb') as audio_file:
            transcript = self.client.audio.transcriptions.create(
                model="whisper-1",
                file=audio_file,
                language="ko"  # í•œêµ­ì–´ ìµœì í™”
            )

        print(f"âœ… ë³€í™˜ ì™„ë£Œ ({len(transcript.text)} ì)")
        return transcript.text

    def summarize_meeting(
        self,
        transcript: str,
        meeting_title: str = "",
        attendees: List[str] = None
    ) -> Dict[str, any]:
        """
        íšŒì˜ë¡ ìš”ì•½

        Args:
            transcript: íšŒì˜ ë‚´ìš© (í…ìŠ¤íŠ¸)
            meeting_title: íšŒì˜ ì œëª©
            attendees: ì°¸ì„ì ëª©ë¡

        Returns:
            êµ¬ì¡°í™”ëœ íšŒì˜ë¡
        """

        system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ íšŒì˜ë¡ ì‘ì„±ìì…ë‹ˆë‹¤.
íšŒì˜ ë‚´ìš©ì„ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.

íšŒì˜ë¡ ì‘ì„± ì›ì¹™:
1. ê°ê´€ì  ì‚¬ì‹¤ë§Œ ê¸°ë¡
2. ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë¦¬
3. ì•¡ì…˜ ì•„ì´í…œì€ ëª…í™•í•˜ê²Œ (ë‹´ë‹¹ì, ê¸°í•œ í¬í•¨)
4. êµ¬ì–´ì²´ë¥¼ ë¬¸ì–´ì²´ë¡œ ë³€í™˜"""

        prompt = f"""ë‹¤ìŒ íšŒì˜ ë‚´ìš©ì„ êµ¬ì¡°í™”ëœ íšŒì˜ë¡ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

**íšŒì˜ ì •ë³´:**
- ì œëª©: {meeting_title or 'ë¯¸ì •'}
- ì°¸ì„ì: {', '.join(attendees) if attendees else 'ë¯¸ê¸°ì¬'}

**íšŒì˜ ë‚´ìš©:**
{transcript}

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "title": "íšŒì˜ ì œëª© (ë‚´ìš©ì—ì„œ ì¶”ë¡ )",
  "summary": "íšŒì˜ ìš”ì•½ (2-3ë¬¸ì¥)",
  "key_points": [
    "ì£¼ìš” ë…¼ì˜ ì‚¬í•­ 1",
    "ì£¼ìš” ë…¼ì˜ ì‚¬í•­ 2",
    ...
  ],
  "decisions": [
    "ê²°ì • ì‚¬í•­ 1",
    "ê²°ì • ì‚¬í•­ 2",
    ...
  ],
  "action_items": [
    {{
      "task": "ìˆ˜í–‰í•  ì‘ì—…",
      "assignee": "ë‹´ë‹¹ì (íšŒì˜ ë‚´ìš©ì—ì„œ ì¶”ì¶œ, ì—†ìœ¼ë©´ TBD)",
      "due_date": "ê¸°í•œ (íšŒì˜ ë‚´ìš©ì—ì„œ ì¶”ì¶œ, ì—†ìœ¼ë©´ TBD)",
      "priority": "High/Medium/Low"
    }}
  ],
  "next_meeting": "ë‹¤ìŒ íšŒì˜ ì¼ì • (ì–¸ê¸‰ëœ ê²½ìš°, ì—†ìœ¼ë©´ null)"
}}

íšŒì˜ ë‚´ìš©ì—ì„œ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”. ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
"""

        response = self.client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,  # ë‚®ì€ ì°½ì˜ì„± (ì •í™•ì„± ìš°ì„ )
            max_tokens=2000,
            response_format={"type": "json_object"}  # JSON ëª¨ë“œ
        )

        # JSON íŒŒì‹±
        result_text = response.choices[0].message.content.strip()
        summary = json.loads(result_text)

        return summary

    def format_meeting_minutes(
        self,
        summary: Dict[str, any],
        transcript: str = ""
    ) -> str:
        """
        íšŒì˜ë¡ì„ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Args:
            summary: ìš”ì•½ëœ íšŒì˜ë¡
            transcript: ì›ë³¸ íšŒì˜ ë‚´ìš© (ì„ íƒ)

        Returns:
            Markdown í˜•ì‹ íšŒì˜ë¡
        """

        # í—¤ë”
        minutes = f"""# {summary['title']}

---

**íšŒì˜ ì •ë³´**
- ì¼ì‹œ: {datetime.now().strftime("%Y-%m-%d")}
- ìš”ì•½ ìƒì„±: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

---

## ìš”ì•½
{summary['summary']}

---

## ì£¼ìš” ë…¼ì˜ ì‚¬í•­
"""

        # ì£¼ìš” ë…¼ì˜ ì‚¬í•­
        for i, point in enumerate(summary['key_points'], 1):
            minutes += f"{i}. {point}\n"

        # ê²°ì • ì‚¬í•­
        if summary.get('decisions'):
            minutes += "\n---\n\n## ê²°ì • ì‚¬í•­\n\n"
            for i, decision in enumerate(summary['decisions'], 1):
                minutes += f"{i}. {decision}\n"

        # ì•¡ì…˜ ì•„ì´í…œ
        if summary.get('action_items'):
            minutes += "\n---\n\n## ì•¡ì…˜ ì•„ì´í…œ\n\n"
            minutes += "| ì‘ì—… | ë‹´ë‹¹ì | ê¸°í•œ | ìš°ì„ ìˆœìœ„ |\n"
            minutes += "|------|--------|------|----------|\n"

            for item in summary['action_items']:
                priority_emoji = {
                    'High': 'ğŸ”´',
                    'Medium': 'ğŸŸ¡',
                    'Low': 'ğŸŸ¢'
                }.get(item.get('priority', 'Medium'), 'âšª')

                minutes += f"| {item['task']} | {item.get('assignee', 'TBD')} | {item.get('due_date', 'TBD')} | {priority_emoji} {item.get('priority', 'Medium')} |\n"

        # ë‹¤ìŒ íšŒì˜
        if summary.get('next_meeting'):
            minutes += f"\n---\n\n## ë‹¤ìŒ íšŒì˜\n{summary['next_meeting']}\n"

        # ì›ë³¸ íšŒì˜ë¡ (ì„ íƒ)
        if transcript:
            minutes += f"\n---\n\n## ì›ë³¸ íšŒì˜ ë‚´ìš©\n\n<details>\n<summary>í¼ì³ë³´ê¸°</summary>\n\n{transcript}\n\n</details>\n"

        return minutes

    def process_meeting(
        self,
        input_path: str,
        meeting_title: str = "",
        attendees: List[str] = None,
        include_transcript: bool = False
    ) -> str:
        """
        íšŒì˜ ì²˜ë¦¬ (ìŒì„± or í…ìŠ¤íŠ¸)

        Args:
            input_path: ì…ë ¥ íŒŒì¼ (ìŒì„± or í…ìŠ¤íŠ¸)
            meeting_title: íšŒì˜ ì œëª©
            attendees: ì°¸ì„ì
            include_transcript: ì›ë³¸ í¬í•¨ ì—¬ë¶€

        Returns:
            Markdown íšŒì˜ë¡
        """

        file_path = Path(input_path)

        # íŒŒì¼ íƒ€ì… í™•ì¸
        audio_extensions = ['.mp3', '.wav', '.m4a', '.ogg', '.webm']
        text_extensions = ['.txt', '.md']

        if file_path.suffix.lower() in audio_extensions:
            # ìŒì„± íŒŒì¼ â†’ í…ìŠ¤íŠ¸ ë³€í™˜
            transcript = self.transcribe_audio(input_path)
        elif file_path.suffix.lower() in text_extensions:
            # í…ìŠ¤íŠ¸ íŒŒì¼ ì§ì ‘ ì½ê¸°
            with open(input_path, 'r', encoding='utf-8') as f:
                transcript = f.read()
            print(f"ğŸ“„ í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì™„ë£Œ ({len(transcript)} ì)")
        else:
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}\n"
                f"ì§€ì› í˜•ì‹: {audio_extensions + text_extensions}"
            )

        # íšŒì˜ë¡ ìš”ì•½
        print("ğŸ“ íšŒì˜ë¡ ìš”ì•½ ì¤‘...")
        summary = self.summarize_meeting(
            transcript,
            meeting_title=meeting_title,
            attendees=attendees
        )

        # Markdown ë³€í™˜
        minutes = self.format_meeting_minutes(
            summary,
            transcript=transcript if include_transcript else ""
        )

        print("âœ… íšŒì˜ë¡ ìƒì„± ì™„ë£Œ")
        return minutes

def main():
    """CLI ì§„ì…ì """

    parser = argparse.ArgumentParser(
        description='AI ê¸°ë°˜ íšŒì˜ë¡ ìë™ ìš”ì•½ê¸°'
    )
    parser.add_argument(
        '--input',
        required=True,
        help='ì…ë ¥ íŒŒì¼ (ìŒì„± íŒŒì¼ or í…ìŠ¤íŠ¸ íŒŒì¼)'
    )
    parser.add_argument(
        '--title',
        default='',
        help='íšŒì˜ ì œëª©'
    )
    parser.add_argument(
        '--attendees',
        default='',
        help='ì°¸ì„ì (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: í™ê¸¸ë™,ê¹€ì² ìˆ˜,ì´ì˜í¬)'
    )
    parser.add_argument(
        '--include-transcript',
        action='store_true',
        help='ì›ë³¸ íšŒì˜ ë‚´ìš© í¬í•¨'
    )
    parser.add_argument(
        '--output',
        default='Meeting_Minutes_{timestamp}.md',
        help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ'
    )
    parser.add_argument(
        '--api-key',
        help='OpenAI API í‚¤'
    )

    args = parser.parse_args()

    # API í‚¤
    import os
    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # ì°¸ì„ì íŒŒì‹±
    attendees = [a.strip() for a in args.attendees.split(',')] if args.attendees else None

    # íšŒì˜ë¡ ìƒì„±
    summarizer = MeetingSummarizer(api_key=api_key)

    print(f"ğŸš€ íšŒì˜ë¡ ìƒì„± ì‹œì‘...")
    print(f"  ì…ë ¥: {args.input}")
    if args.title:
        print(f"  ì œëª©: {args.title}")
    if attendees:
        print(f"  ì°¸ì„ì: {', '.join(attendees)}")
    print()

    minutes = summarizer.process_meeting(
        input_path=args.input,
        meeting_title=args.title,
        attendees=attendees,
        include_transcript=args.include_transcript
    )

    # ì¶œë ¥ íŒŒì¼ëª…
    if '{timestamp}' in args.output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = args.output.replace('{timestamp}', timestamp)
    else:
        output_path = args.output

    # ì €ì¥
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(minutes)

    print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")

if __name__ == '__main__':
    main()
```

#### 5.2.3 ì‚¬ìš© ì˜ˆì‹œ

```bash
# ìŒì„± íŒŒì¼ ì²˜ë¦¬
python meeting_summarizer.py \\
  --input meeting_20250115.mp3 \\
  --title "Q1 ì œí’ˆ ë¡œë“œë§µ íšŒì˜" \\
  --attendees "ê¹€PM,ì´ê°œë°œì,ë°•ë””ìì´ë„ˆ"

# í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
python meeting_summarizer.py \\
  --input meeting_transcript.txt \\
  --title "ì£¼ê°„ ìŠ¤í”„ë¦°íŠ¸ ë¦¬ë·°" \\
  --include-transcript

# ê°„ë‹¨ ì‚¬ìš© (ì œëª© ìë™ ì¶”ì¶œ)
python meeting_summarizer.py --input meeting.txt
```

---

### 5.3 ë„êµ¬ 3: ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„±ê¸° ğŸ‘¤

#### 5.3.1 ëª©í‘œ ë° ê¸°ëŒ€ íš¨ê³¼

**ëª©í‘œ:**
- ê¸°ëŠ¥ ì„¤ëª… â†’ ì™„ì „í•œ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ì„¸íŠ¸
- ë‹¤ì–‘í•œ ì‚¬ìš©ì í˜ë¥´ì†Œë‚˜ ê³ ë ¤
- ì‘ì„± ì‹œê°„ 70% ì ˆê°

**ê¸°ëŒ€ íš¨ê³¼:**
- ì‚¬ìš©ì ê´€ì  ì‚¬ê³  ê°•í™”
- Edge case ëˆ„ë½ ë°©ì§€
- ê°œë°œíŒ€ê³¼ì˜ ì†Œí†µ ê°œì„ 

#### 5.3.2 êµ¬í˜„ (ì™„ì „í•œ ì½”ë“œ)

```python
# user_story_generator.py
"""
ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„±ê¸°

ì‚¬ìš©ë²•:
    python user_story_generator.py --feature "ì•Œë¦¼ ì„¤ì •" --personas "ì§ì¥ì¸,í•™ìƒ"
"""

from openai import OpenAI
from typing import List, Dict
import argparse
import json
from datetime import datetime

class UserStoryGenerator:
    """ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„±ê¸°"""

    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)

        # ê¸°ë³¸ í˜ë¥´ì†Œë‚˜ í…œí”Œë¦¿
        self.default_personas = {
            "ì§ì¥ì¸": "25-45ì„¸, ì—…ë¬´ ì¤‘ì‹¬, íš¨ìœ¨ì„± ì¤‘ì‹œ, ì œí•œëœ ì‹œê°„",
            "í•™ìƒ": "18-25ì„¸, í•™ì—… ì¤‘ì‹¬, ë¹„ìš© ë¯¼ê°, ëª¨ë°”ì¼ ì„ í˜¸",
            "ì‹œë‹ˆì–´": "50ì„¸ ì´ìƒ, ê¸°ìˆ  ì¹œìˆ™ë„ ë‚®ìŒ, ë‹¨ìˆœí•¨ ì„ í˜¸",
            "íŒŒì›Œìœ ì €": "ì–¼ë¦¬ì–´ë‹µí„°, ê³ ê¸‰ ê¸°ëŠ¥ ì„ í˜¸, ì»¤ìŠ¤í„°ë§ˆì´ì§• ì¤‘ìš”"
        }

    def generate_user_stories(
        self,
        feature_description: str,
        personas: List[str] = None,
        acceptance_criteria: bool = True
    ) -> List[Dict[str, any]]:
        """
        ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„±

        Args:
            feature_description: ê¸°ëŠ¥ ì„¤ëª…
            personas: í˜ë¥´ì†Œë‚˜ ëª©ë¡ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            acceptance_criteria: ìˆ˜ë½ ê¸°ì¤€ í¬í•¨ ì—¬ë¶€

        Returns:
            ì‚¬ìš©ì ìŠ¤í† ë¦¬ ë¦¬ìŠ¤íŠ¸
        """

        # í˜ë¥´ì†Œë‚˜ ì„ íƒ
        if not personas:
            personas = list(self.default_personas.keys())

        persona_context = "\n".join([
            f"- {p}: {self.default_personas.get(p, 'ì¼ë°˜ ì‚¬ìš©ì')}"
            for p in personas
        ])

        system_prompt = """ë‹¹ì‹ ì€ ì‚¬ìš©ì ê²½í—˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ì–‘í•œ ì‚¬ìš©ì ê´€ì ì—ì„œ ìŠ¤í† ë¦¬ë¥¼ ì‘ì„±í•©ë‹ˆë‹¤.

ìŠ¤í† ë¦¬ ì‘ì„± ì›ì¹™:
1. ì‚¬ìš©ì ê°€ì¹˜ ì¤‘ì‹¬ (Whyê°€ ëª…í™•)
2. í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ (ìˆ˜ë½ ê¸°ì¤€ êµ¬ì²´ì )
3. ë…ë¦½ì  (ë‹¤ë¥¸ ìŠ¤í† ë¦¬ì™€ ì˜ì¡´ì„± ìµœì†Œí™”)
4. í˜‘ìƒ ê°€ëŠ¥ (êµ¬í˜„ ë°©ë²•ì€ ìœ ì—°í•˜ê²Œ)"""

        prompt = f"""ë‹¤ìŒ ê¸°ëŠ¥ì— ëŒ€í•œ ì‚¬ìš©ì ìŠ¤í† ë¦¬ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

**ê¸°ëŠ¥:** {feature_description}

**í˜ë¥´ì†Œë‚˜:**
{persona_context}

**ìš”êµ¬ì‚¬í•­:**
- ê° í˜ë¥´ì†Œë‚˜ë‹¹ ìµœì†Œ 2ê°œ ìŠ¤í† ë¦¬
- ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ í¬í•¨ (Happy path + Edge cases)
- ìš°ì„ ìˆœìœ„ ì§€ì • (Must-have / Should-have / Nice-to-have)

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "stories": [
    {{
      "persona": "í˜ë¥´ì†Œë‚˜ëª…",
      "story": "As a [ì—­í• ], I want [ê¸°ëŠ¥], so that [ì´ìœ ]",
      "priority": "Must-have | Should-have | Nice-to-have",
      "acceptance_criteria": [
        "Given [ì „ì œ], When [í–‰ë™], Then [ê²°ê³¼]",
        ...
      ],
      "notes": "ì¶”ê°€ ê³ ë ¤ì‚¬í•­ (ì„ íƒ)"
    }}
  ]
}}

ì‹¤ì œ ì‚¬ìš©ì ê´€ì ì—ì„œ ì‘ì„±í•˜ì„¸ìš”.
"""

        response = self.client.chat.completions.create(
            model="gpt-4-turbo-preview",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8,  # ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì•½ê°„ ë†’ê²Œ
            max_tokens=2500,
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)
        return result['stories']

    def format_user_stories(
        self,
        stories: List[Dict[str, any]],
        format: str = 'markdown'
    ) -> str:
        """
        ì‚¬ìš©ì ìŠ¤í† ë¦¬ í¬ë§·íŒ…

        Args:
            stories: ìŠ¤í† ë¦¬ ë¦¬ìŠ¤íŠ¸
            format: ì¶œë ¥ í˜•ì‹ (markdown, jira, csv)

        Returns:
            í¬ë§·íŒ…ëœ ë¬¸ìì—´
        """

        if format == 'markdown':
            return self._format_markdown(stories)
        elif format == 'jira':
            return self._format_jira(stories)
        elif format == 'csv':
            return self._format_csv(stories)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í˜•ì‹: {format}")

    def _format_markdown(self, stories: List[Dict]) -> str:
        """Markdown í˜•ì‹"""

        output = f"""# ì‚¬ìš©ì ìŠ¤í† ë¦¬

**ìƒì„± ì¼ì‹œ:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**ì´ ìŠ¤í† ë¦¬ ìˆ˜:** {len(stories)}

---

"""

        # ìš°ì„ ìˆœìœ„ë³„ ê·¸ë£¹í™”
        by_priority = {}
        for story in stories:
            priority = story['priority']
            if priority not in by_priority:
                by_priority[priority] = []
            by_priority[priority].append(story)

        # ì¶œë ¥
        priority_order = ['Must-have', 'Should-have', 'Nice-to-have']
        for priority in priority_order:
            if priority not in by_priority:
                continue

            priority_emoji = {
                'Must-have': 'ğŸ”´',
                'Should-have': 'ğŸŸ¡',
                'Nice-to-have': 'ğŸŸ¢'
            }[priority]

            output += f"## {priority_emoji} {priority}\n\n"

            for i, story in enumerate(by_priority[priority], 1):
                output += f"### {i}. [{story['persona']}] {story['story']}\n\n"

                output += "**ìŠ¤í† ë¦¬:**\n"
                output += f"> {story['story']}\n\n"

                if story.get('acceptance_criteria'):
                    output += "**ìˆ˜ë½ ê¸°ì¤€:**\n"
                    for j, criteria in enumerate(story['acceptance_criteria'], 1):
                        output += f"{j}. {criteria}\n"
                    output += "\n"

                if story.get('notes'):
                    output += f"**ì°¸ê³ :**\n{story['notes']}\n\n"

                output += "---\n\n"

        return output

    def _format_jira(self, stories: List[Dict]) -> str:
        """Jira import í˜•ì‹ (ê°„ë‹¨ ë²„ì „)"""

        output = "Summary|Description|Priority|Acceptance Criteria\n"

        for story in stories:
            summary = f"[{story['persona']}] {story['story'][:50]}..."
            description = story['story']
            priority = story['priority']

            criteria = "\\n".join([
                f"- {c}" for c in story.get('acceptance_criteria', [])
            ])

            output += f"{summary}|{description}|{priority}|{criteria}\n"

        return output

    def _format_csv(self, stories: List[Dict]) -> str:
        """CSV í˜•ì‹"""

        import csv
        from io import StringIO

        output = StringIO()
        writer = csv.writer(output)

        # í—¤ë”
        writer.writerow(['Persona', 'Story', 'Priority', 'Acceptance Criteria', 'Notes'])

        # ë°ì´í„°
        for story in stories:
            criteria = "; ".join(story.get('acceptance_criteria', []))
            writer.writerow([
                story['persona'],
                story['story'],
                story['priority'],
                criteria,
                story.get('notes', '')
            ])

        return output.getvalue()

def main():
    """CLI ì§„ì…ì """

    parser = argparse.ArgumentParser(
        description='AI ê¸°ë°˜ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„±ê¸°'
    )
    parser.add_argument(
        '--feature',
        required=True,
        help='ê¸°ëŠ¥ ì„¤ëª…'
    )
    parser.add_argument(
        '--personas',
        default='',
        help='í˜ë¥´ì†Œë‚˜ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì˜ˆ: ì§ì¥ì¸,í•™ìƒ,ì‹œë‹ˆì–´)'
    )
    parser.add_argument(
        '--format',
        choices=['markdown', 'jira', 'csv'],
        default='markdown',
        help='ì¶œë ¥ í˜•ì‹'
    )
    parser.add_argument(
        '--output',
        default='User_Stories_{timestamp}.md',
        help='ì¶œë ¥ íŒŒì¼'
    )
    parser.add_argument(
        '--api-key',
        help='OpenAI API í‚¤'
    )

    args = parser.parse_args()

    # API í‚¤
    import os
    api_key = args.api_key or os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    # í˜ë¥´ì†Œë‚˜ íŒŒì‹±
    personas = [p.strip() for p in args.personas.split(',')] if args.personas else None

    # ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„±
    generator = UserStoryGenerator(api_key=api_key)

    print("ğŸš€ ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„± ì¤‘...")
    print(f"  ê¸°ëŠ¥: {args.feature}")
    if personas:
        print(f"  í˜ë¥´ì†Œë‚˜: {', '.join(personas)}")
    print()

    stories = generator.generate_user_stories(
        feature_description=args.feature,
        personas=personas
    )

    print(f"âœ… ì´ {len(stories)}ê°œ ìŠ¤í† ë¦¬ ìƒì„± ì™„ë£Œ")

    # í¬ë§·íŒ…
    formatted = generator.format_user_stories(stories, format=args.format)

    # ì €ì¥
    if '{timestamp}' in args.output:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = args.output.replace('{timestamp}', timestamp)
    else:
        output_path = args.output

    # í™•ì¥ì ì¡°ì •
    if args.format == 'csv' and not output_path.endswith('.csv'):
        output_path = output_path.replace('.md', '.csv')

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(formatted)

    print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}")

if __name__ == '__main__':
    main()
```

#### 5.3.3 ì‚¬ìš© ì˜ˆì‹œ

```bash
# ê¸°ë³¸ ì‚¬ìš©
python user_story_generator.py --feature "ì•Œë¦¼ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•"

# íŠ¹ì • í˜ë¥´ì†Œë‚˜
python user_story_generator.py \\
  --feature "ë‹¤í¬ëª¨ë“œ ì§€ì›" \\
  --personas "ì§ì¥ì¸,íŒŒì›Œìœ ì €"

# Jira import í˜•ì‹
python user_story_generator.py \\
  --feature "íŒŒì¼ ê³µìœ  ê¸°ëŠ¥" \\
  --format jira \\
  --output stories.txt
```

---

**âœ… Phase 3 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] PRD ìƒì„±ê¸° êµ¬ì¶• ì™„ë£Œ
- [ ] íšŒì˜ë¡ ìš”ì•½ê¸° êµ¬ì¶• ì™„ë£Œ
- [ ] ì‚¬ìš©ì ìŠ¤í† ë¦¬ ìƒì„±ê¸° êµ¬ì¶• ì™„ë£Œ
- [ ] 3ê°€ì§€ ë„êµ¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] ì‹¤ë¬´ ì ìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€

**ë‹¤ìŒ ë‹¨ê³„:** Phase 4ì—ì„œ ê°œë°œììš© ë„êµ¬ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

---

## 6. Phase 4: ê°œë°œììš© ë„êµ¬ êµ¬ì¶•

ì´ ì„¹ì…˜ì—ì„œëŠ” ê°œë°œìì˜ ìƒì‚°ì„±ì„ ë†’ì´ëŠ” 3ê°€ì§€ í•µì‹¬ ë„êµ¬ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

### 6.1 ë„êµ¬ 1: ì½”ë“œ ë¦¬ë·° ìë™í™” ë´‡ ğŸ”

#### 6.1.1 ëª©í‘œ ë° ê¸°ëŒ€ íš¨ê³¼

**ëª©í‘œ:**
- Pull Request ìë™ ë¦¬ë·°
- ì½”ë“œ í’ˆì§ˆ ë¬¸ì œ ì‹ë³„
- ë¦¬ë·° ì‹œê°„ 50% ì ˆê°

**ê¸°ëŒ€ íš¨ê³¼:**
- ì¼ê´€ëœ ë¦¬ë·° ê¸°ì¤€
- ì‹ ì†í•œ í”¼ë“œë°±
- ì‹œë‹ˆì–´ ê°œë°œì ë¶€ë‹´ ê°ì†Œ

#### 6.1.2 í•µì‹¬ êµ¬í˜„ (ìš”ì•½)

```python
# code_reviewer.py
"""
ì½”ë“œ ë¦¬ë·° ìë™í™” ë´‡

GitHub PRì„ ìë™ìœ¼ë¡œ ë¦¬ë·°í•˜ê³  ì½”ë©˜íŠ¸ ì‘ì„±
"""

from anthropic import Anthropic
import os
import subprocess

class CodeReviewer:
    """ì½”ë“œ ë¦¬ë·° ë´‡"""

    def __init__(self, api_key: str):
        self.client = Anthropic(api_key=api_key)

    def review_pull_request(self, diff_text: str, language: str = "python") -> dict:
        """PR ì°¨ì´ì ì„ ë¦¬ë·°"""

        system_prompt = f"""ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ {language} ê°œë°œìì…ë‹ˆë‹¤.

ì½”ë“œ ë¦¬ë·° ì²´í¬ë¦¬ìŠ¤íŠ¸:
1. ë²„ê·¸ ê°€ëŠ¥ì„±
2. ì„±ëŠ¥ ì´ìŠˆ
3. ë³´ì•ˆ ì·¨ì•½ì 
4. ì½”ë“œ ìŠ¤íƒ€ì¼
5. í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€
6. ë¬¸ì„œí™”

ê±´ì„¤ì ì´ê³  ì¹œì ˆí•œ í”¼ë“œë°±ì„ ì œê³µí•˜ì„¸ìš”."""

        prompt = f"""ë‹¤ìŒ ì½”ë“œ ë³€ê²½ì‚¬í•­ì„ ë¦¬ë·°í•˜ì„¸ìš”:

```diff
{diff_text}
```

**ë¦¬ë·° í˜•ì‹ (JSON):**
{{
  "overall_score": "1-10",
  "summary": "ì „ì²´ ìš”ì•½ (2-3ë¬¸ì¥)",
  "issues": [
    {{
      "severity": "critical|high|medium|low",
      "line": "í•´ë‹¹ ë¼ì¸",
      "issue": "ë¬¸ì œ ì„¤ëª…",
      "suggestion": "ê°œì„  ì œì•ˆ"
    }}
  ],
  "positive_points": ["ì˜í•œ ì  1", "ì˜í•œ ì  2"],
  "recommendations": ["ì „ë°˜ì  ê°œì„  ì œì•ˆ"]
}}"""

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=3000,
            messages=[{"role": "user", "content": prompt}],
            system=system_prompt
        )

        import json
        return json.loads(response.content[0].text)

    def get_pr_diff(self, pr_number: str) -> str:
        """GitHub PRì˜ diff ê°€ì ¸ì˜¤ê¸°"""
        cmd = f"gh pr diff {pr_number}"
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout

    def post_review_comment(self, pr_number: str, review: dict):
        """ë¦¬ë·°ë¥¼ GitHub PRì— ì½”ë©˜íŠ¸ë¡œ ì‘ì„±"""

        comment = f"""## ğŸ¤– AI Code Review

**Overall Score:** {review['overall_score']}/10

### Summary
{review['summary']}

### Issues Found
"""

        for issue in review['issues']:
            severity_emoji = {
                'critical': 'ğŸ”´',
                'high': 'ğŸŸ ',
                'medium': 'ğŸŸ¡',
                'low': 'ğŸŸ¢'
            }[issue['severity']]

            comment += f"""
{severity_emoji} **{issue['severity'].upper()}** (Line {issue['line']})
- **Issue:** {issue['issue']}
- **Suggestion:** {issue['suggestion']}
"""

        comment += "\n### Positive Points\n"
        for point in review['positive_points']:
            comment += f"- {point}\n"

        comment += "\n### Recommendations\n"
        for rec in review['recommendations']:
            comment += f"- {rec}\n"

        # GitHub CLIë¡œ ì½”ë©˜íŠ¸ ì‘ì„±
        comment_file = "/tmp/review_comment.md"
        with open(comment_file, 'w') as f:
            f.write(comment)

        cmd = f"gh pr comment {pr_number} --body-file {comment_file}"
        subprocess.run(cmd, shell=True)

        print(f"âœ… Review posted to PR #{pr_number}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--pr", required=True, help="PR number")
    args = parser.parse_args()

    reviewer = CodeReviewer(api_key=os.getenv("ANTHROPIC_API_KEY"))

    print(f"ğŸ” Reviewing PR #{args.pr}...")
    diff = reviewer.get_pr_diff(args.pr)

    review = reviewer.review_pull_request(diff)
    reviewer.post_review_comment(args.pr, review)
```

#### 6.1.3 ì‚¬ìš© ì˜ˆì‹œ

```bash
# PR ë¦¬ë·°
python code_reviewer.py --pr 123

# GitHub Actions í†µí•©
# .github/workflows/ai-review.yml
name: AI Code Review
on: [pull_request]
jobs:
  review:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: AI Review
        run: python code_reviewer.py --pr ${{ github.event.pull_request.number }}
```

---

### 6.2 ë„êµ¬ 2: í…ŒìŠ¤íŠ¸ ì½”ë“œ ìƒì„±ê¸° ğŸ§ª

#### 6.2.1 ëª©í‘œ ë° ê¸°ëŒ€ íš¨ê³¼

**ëª©í‘œ:**
- í•¨ìˆ˜/í´ë˜ìŠ¤ â†’ Unit Test ìë™ ìƒì„±
- í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ í–¥ìƒ
- ì‘ì„± ì‹œê°„ 60% ì ˆê°

**ê¸°ëŒ€ íš¨ê³¼:**
- ëˆ„ë½ëœ edge case ë°œê²¬
- ì¼ê´€ëœ í…ŒìŠ¤íŠ¸ íŒ¨í„´
- ì‹ ê·œ ê°œë°œì í•™ìŠµ ìë£Œ

#### 6.2.2 í•µì‹¬ êµ¬í˜„ (ìš”ì•½)

```python
# test_generator.py
"""
í…ŒìŠ¤íŠ¸ ì½”ë“œ ìë™ ìƒì„±ê¸°

í•¨ìˆ˜/í´ë˜ìŠ¤ ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ Unit Test ìƒì„±
"""

from openai import OpenAI
import ast
import os

class TestGenerator:
    """í…ŒìŠ¤íŠ¸ ì½”ë“œ ìƒì„±ê¸°"""

    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)

    def generate_tests(self, source_code: str, framework: str = "pytest") -> str:
        """ì†ŒìŠ¤ ì½”ë“œì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ ìƒì„±"""

        # í•¨ìˆ˜/í´ë˜ìŠ¤ ì¶”ì¶œ
        functions = self._extract_functions(source_code)

        prompt = f"""ë‹¤ìŒ Python ì½”ë“œì— ëŒ€í•œ {framework} í…ŒìŠ¤íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”:

```python
{source_code}
```

**ìš”êµ¬ì‚¬í•­:**
1. ëª¨ë“  public í•¨ìˆ˜/ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
2. Happy path + Edge cases
3. ì˜ˆì™¸ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
4. Mock í•„ìš” ì‹œ ì‚¬ìš© (unittest.mock)
5. ëª…í™•í•œ í…ŒìŠ¤íŠ¸ ì´ë¦„ (test_í•¨ìˆ˜ëª…_ìƒí™©_ì˜ˆìƒê²°ê³¼)

**í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ëª©í‘œ:** 80% ì´ìƒ

ì™„ì „í•œ í…ŒìŠ¤íŠ¸ íŒŒì¼ì„ ì‘ì„±í•˜ì„¸ìš” (import í¬í•¨):"""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ í…ŒìŠ¤íŠ¸ ì£¼ë„ ê°œë°œ(TDD) ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )

        test_code = response.choices[0].message.content

        # ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
        if "```python" in test_code:
            test_code = test_code.split("```python")[1].split("```")[0].strip()

        return test_code

    def _extract_functions(self, source_code: str) -> list:
        """ì†ŒìŠ¤ ì½”ë“œì—ì„œ í•¨ìˆ˜ ì¶”ì¶œ"""
        try:
            tree = ast.parse(source_code)
            functions = []

            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(node.name)

            return functions
        except:
            return []

    def save_test_file(self, source_file: str, test_code: str):
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ì €ì¥"""
        # test_xxx.py í˜•ì‹
        if source_file.startswith("test_"):
            test_file = source_file
        else:
            base_name = os.path.basename(source_file)
            test_file = f"test_{base_name}"

        test_dir = os.path.join(os.path.dirname(source_file), "tests")
        os.makedirs(test_dir, exist_ok=True)

        test_path = os.path.join(test_dir, test_file)

        with open(test_path, 'w') as f:
            f.write(test_code)

        print(f"âœ… Test file created: {test_path}")
        return test_path

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--file", required=True, help="Source code file")
    parser.add_argument("--framework", default="pytest", choices=["pytest", "unittest"])
    args = parser.parse_args()

    generator = TestGenerator(api_key=os.getenv("OPENAI_API_KEY"))

    with open(args.file, 'r') as f:
        source_code = f.read()

    print(f"ğŸ§ª Generating tests for {args.file}...")
    test_code = generator.generate_tests(source_code, framework=args.framework)

    test_path = generator.save_test_file(args.file, test_code)

    print(f"\nğŸ“Š Run tests:")
    print(f"  pytest {test_path} -v")
```

#### 6.2.3 ì‚¬ìš© ì˜ˆì‹œ

```bash
# í…ŒìŠ¤íŠ¸ ìƒì„±
python test_generator.py --file user_service.py

# ìƒì„±ëœ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest tests/test_user_service.py -v --cov=user_service
```

---

### 6.3 ë„êµ¬ 3: API ë¬¸ì„œ ìë™ ìƒì„±ê¸° ğŸ“š

#### 6.3.1 ëª©í‘œ ë° ê¸°ëŒ€ íš¨ê³¼

**ëª©í‘œ:**
- ì†ŒìŠ¤ ì½”ë“œ â†’ OpenAPI/Swagger ë¬¸ì„œ
- ë¬¸ì„œ ë™ê¸°í™” ìë™í™”
- ì‘ì„± ì‹œê°„ 80% ì ˆê°

**ê¸°ëŒ€ íš¨ê³¼:**
- ìµœì‹  ë¬¸ì„œ ìœ ì§€
- API ì‚¬ìš©ì„± í–¥ìƒ
- í”„ë¡ íŠ¸ì—”ë“œ íŒ€ê³¼ì˜ í˜‘ì—… ê°œì„ 

#### 6.3.2 í•µì‹¬ êµ¬í˜„ (ìš”ì•½)

```python
# api_doc_generator.py
"""
API ë¬¸ì„œ ìë™ ìƒì„±ê¸°

FastAPI/Flask ì½”ë“œ â†’ OpenAPI ë¬¸ì„œ + ì‚¬ìš© ì˜ˆì‹œ
"""

from openai import OpenAI
import os
import json

class APIDocGenerator:
    """API ë¬¸ì„œ ìƒì„±ê¸°"""

    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key)

    def generate_api_docs(self, source_code: str, framework: str = "fastapi") -> dict:
        """API ì½”ë“œ â†’ ë¬¸ì„œ ìƒì„±"""

        prompt = f"""ë‹¤ìŒ {framework} ì½”ë“œë¥¼ ë¶„ì„í•˜ì—¬ API ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”:

```python
{source_code}
```

**ì¶œë ¥ í˜•ì‹ (JSON):**
{{
  "endpoints": [
    {{
      "method": "GET|POST|PUT|DELETE",
      "path": "/api/users",
      "summary": "ê°„ë‹¨í•œ ì„¤ëª…",
      "description": "ìƒì„¸ ì„¤ëª…",
      "parameters": [
        {{
          "name": "íŒŒë¼ë¯¸í„°ëª…",
          "type": "string|integer|...",
          "required": true|false,
          "description": "ì„¤ëª…"
        }}
      ],
      "request_body": {{
        "content_type": "application/json",
        "schema": {{"í•„ë“œ": "íƒ€ì…"}},
        "example": {{"ìƒ˜í”Œ": "ë°ì´í„°"}}
      }},
      "responses": {{
        "200": {{
          "description": "ì„±ê³µ",
          "example": {{"result": "success"}}
        }},
        "400": {{
          "description": "ì‹¤íŒ¨",
          "example": {{"error": "message"}}
        }}
      }},
      "curl_example": "curl ëª…ë ¹ì–´ ì˜ˆì‹œ"
    }}
  ]
}}

ëª¨ë“  ì—”ë“œí¬ì¸íŠ¸ë¥¼ í¬í•¨í•˜ì„¸ìš”."""

        response = self.client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=3000,
            response_format={"type": "json_object"}
        )

        return json.loads(response.choices[0].message.content)

    def generate_markdown_docs(self, api_docs: dict) -> str:
        """Markdown ë¬¸ì„œ ìƒì„±"""

        md = "# API Documentation\n\n"
        md += "## Endpoints\n\n"

        for endpoint in api_docs['endpoints']:
            md += f"### {endpoint['method']} `{endpoint['path']}`\n\n"
            md += f"{endpoint['summary']}\n\n"
            md += f"**Description:** {endpoint['description']}\n\n"

            # Parameters
            if endpoint.get('parameters'):
                md += "**Parameters:**\n\n"
                md += "| Name | Type | Required | Description |\n"
                md += "|------|------|----------|-------------|\n"
                for param in endpoint['parameters']:
                    req = "Yes" if param['required'] else "No"
                    md += f"| {param['name']} | {param['type']} | {req} | {param['description']} |\n"
                md += "\n"

            # Request Body
            if endpoint.get('request_body'):
                body = endpoint['request_body']
                md += "**Request Body:**\n\n"
                md += f"```json\n{json.dumps(body['example'], indent=2)}\n```\n\n"

            # Responses
            if endpoint.get('responses'):
                md += "**Responses:**\n\n"
                for status, resp in endpoint['responses'].items():
                    md += f"- **{status}:** {resp['description']}\n"
                    md += f"  ```json\n  {json.dumps(resp['example'], indent=2)}\n  ```\n\n"

            # cURL Example
            if endpoint.get('curl_example'):
                md += "**Example:**\n\n"
                md += f"```bash\n{endpoint['curl_example']}\n```\n\n"

            md += "---\n\n"

        return md

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--file", required=True, help="API source file")
    parser.add_argument("--output", default="API_DOCS.md")
    args = parser.parse_args()

    generator = APIDocGenerator(api_key=os.getenv("OPENAI_API_KEY"))

    with open(args.file, 'r') as f:
        source_code = f.read()

    print("ğŸ“š Generating API documentation...")
    api_docs = generator.generate_api_docs(source_code)
    markdown_docs = generator.generate_markdown_docs(api_docs)

    with open(args.output, 'w') as f:
        f.write(markdown_docs)

    print(f"âœ… Documentation saved: {args.output}")
```

---

**âœ… Phase 4 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ì½”ë“œ ë¦¬ë·° ë´‡ êµ¬ì¶• ì™„ë£Œ
- [ ] í…ŒìŠ¤íŠ¸ ìƒì„±ê¸° êµ¬ì¶• ì™„ë£Œ
- [ ] API ë¬¸ì„œ ìƒì„±ê¸° êµ¬ì¶• ì™„ë£Œ
- [ ] 3ê°€ì§€ ë„êµ¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ
- [ ] CI/CD í†µí•© ê°€ëŠ¥

**ë‹¤ìŒ ë‹¨ê³„:** Phase 5ì—ì„œ ì‹¤ì „ ë°°í¬ ë° ìš´ì˜ ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

---

## 7. Phase 5: ì‹¤ì „ êµ¬ì¶• ë° ë°°í¬

### 7.1 í”„ë¡œë•ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸ âœ…

#### 7.1.1 ë³´ì•ˆ

**í•„ìˆ˜ ë³´ì•ˆ ì¡°ì¹˜:**
- âœ… API í‚¤ í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬ (.env + .gitignore)
- âœ… Rate limiting ì ìš© (AI API í˜¸ì¶œ ì œí•œ)
- âœ… ì…ë ¥ ê²€ì¦ (SQL Injection, XSS ë°©ì§€)
- âœ… ë¡œê·¸ì— ë¯¼ê° ì •ë³´ ì œì™¸
- âœ… HTTPS ì‚¬ìš© (ì›¹ ì„œë¹„ìŠ¤ ì‹œ)

```python
# security_utils.py
"""ë³´ì•ˆ ìœ í‹¸ë¦¬í‹°"""

import os
import re
from functools import wraps
import time

class RateLimiter:
    """ê°„ë‹¨í•œ Rate Limiter"""

    def __init__(self, max_calls: int, period: int):
        self.max_calls = max_calls
        self.period = period  # seconds
        self.calls = []

    def __call__(self, func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            now = time.time()

            # ì˜¤ë˜ëœ í˜¸ì¶œ ì œê±°
            self.calls = [c for c in self.calls if c > now - self.period]

            if len(self.calls) >= self.max_calls:
                raise Exception(
                    f"Rate limit exceeded: {self.max_calls} calls per {self.period}s"
                )

            self.calls.append(now)
            return func(*args, **kwargs)

        return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
@RateLimiter(max_calls=10, period=60)  # ë¶„ë‹¹ 10íšŒ
def ai_call(prompt):
    # AI API í˜¸ì¶œ
    pass
```

#### 7.1.2 ëª¨ë‹ˆí„°ë§

**í•µì‹¬ ì§€í‘œ:**
- API í˜¸ì¶œ íšŸìˆ˜ ë° ë¹„ìš©
- ì‘ë‹µ ì‹œê°„ (p50, p95, p99)
- ì—ëŸ¬ìœ¨
- ì‚¬ìš©ì ë§Œì¡±ë„

```python
# monitoring.py
"""ê°„ë‹¨í•œ ëª¨ë‹ˆí„°ë§"""

import time
import json
from datetime import datetime

class Monitor:
    """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

    def __init__(self, log_file="metrics.jsonl"):
        self.log_file = log_file

    def log_api_call(self, model: str, tokens: int, latency: float, success: bool):
        """API í˜¸ì¶œ ë¡œê·¸"""
        metric = {
            "timestamp": datetime.now().isoformat(),
            "model": model,
            "tokens": tokens,
            "latency_ms": latency * 1000,
            "success": success
        }

        with open(self.log_file, 'a') as f:
            f.write(json.dumps(metric) + "\n")

    def get_daily_stats(self):
        """ì¼ì¼ í†µê³„"""
        # ê°„ë‹¨í•œ ì§‘ê³„ ë¡œì§
        pass

# ì‚¬ìš© ì˜ˆì‹œ
monitor = Monitor()

start = time.time()
try:
    response = ai_call(prompt)
    monitor.log_api_call("gpt-4", response.usage.total_tokens, time.time() - start, True)
except Exception as e:
    monitor.log_api_call("gpt-4", 0, time.time() - start, False)
```

#### 7.1.3 ë¹„ìš© ê´€ë¦¬

**ë¹„ìš© ì ˆê° ì „ëµ:**
1. **ëª¨ë¸ ì„ íƒ**: ê°„ë‹¨í•œ ì‘ì—…ì€ ì €ë ´í•œ ëª¨ë¸
2. **ìºì‹±**: ë™ì¼ ìš”ì²­ ì¬ì‚¬ìš©
3. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ ë¬¶ì–´ì„œ ì²˜ë¦¬
4. **í† í° ìµœì í™”**: ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°

```python
# cost_optimizer.py
"""ë¹„ìš© ìµœì í™”"""

def estimate_monthly_cost(
    daily_calls: int,
    avg_input_tokens: int,
    avg_output_tokens: int,
    model: str = "gpt-4"
):
    """ì›”ê°„ ë¹„ìš© ì˜ˆì¸¡"""

    prices = {
        "gpt-4": {"input": 0.03, "output": 0.06},
        "gpt-3.5-turbo": {"input": 0.001, "output": 0.002},
        "claude-3-5-sonnet": {"input": 0.03, "output": 0.15}
    }

    price = prices.get(model, prices["gpt-4"])

    daily_input_cost = (daily_calls * avg_input_tokens / 1000) * price["input"]
    daily_output_cost = (daily_calls * avg_output_tokens / 1000) * price["output"]

    monthly_cost = (daily_input_cost + daily_output_cost) * 30

    print(f"ğŸ“Š ì˜ˆìƒ ì›”ê°„ ë¹„ìš© ({model}):")
    print(f"  - ì¼ì¼ í˜¸ì¶œ: {daily_calls}íšŒ")
    print(f"  - í‰ê·  ì…ë ¥: {avg_input_tokens} tokens")
    print(f"  - í‰ê·  ì¶œë ¥: {avg_output_tokens} tokens")
    print(f"  - ì›”ê°„ ë¹„ìš©: ${monthly_cost:.2f}")

    return monthly_cost

# ì‚¬ìš© ì˜ˆì‹œ
estimate_monthly_cost(
    daily_calls=100,
    avg_input_tokens=500,
    avg_output_tokens=1000,
    model="gpt-4"
)
```

### 7.2 ë°°í¬ ì „ëµ

#### 7.2.1 CLI ë„êµ¬ ë°°í¬

```bash
# pyproject.toml (Poetry ì‚¬ìš©)
[tool.poetry]
name = "ai-productivity-tools"
version = "1.0.0"
description = "AI-powered productivity tools"

[tool.poetry.scripts]
prd-gen = "ai_tools.prd_generator:main"
meeting-sum = "ai_tools.meeting_summarizer:main"
code-review = "ai_tools.code_reviewer:main"

# ì„¤ì¹˜
pip install -e .

# ì‚¬ìš©
prd-gen --idea "ìƒˆ ê¸°ëŠ¥" --users "ì‚¬ìš©ì" --goal "ëª©í‘œ"
```

#### 7.2.2 ì›¹ ì„œë¹„ìŠ¤ ë°°í¬ (FastAPI)

```python
# app.py
"""ì›¹ ì„œë¹„ìŠ¤ ë²„ì „"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os

app = FastAPI(title="AI Productivity Tools")

class PRDRequest(BaseModel):
    feature_idea: str
    target_users: str
    business_goal: str

@app.post("/api/generate-prd")
async def generate_prd(request: PRDRequest):
    """PRD ìƒì„± API"""
    try:
        from prd_generator import PRDGenerator

        generator = PRDGenerator(api_key=os.getenv("OPENAI_API_KEY"))
        sections = generator.generate_prd(
            feature_idea=request.feature_idea,
            target_users=request.target_users,
            business_goal=request.business_goal
        )

        return {"status": "success", "prd": sections}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ì‹¤í–‰
# uvicorn app:app --reload
```

```bash
# Docker ë°°í¬
# Dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

CMD ["uvicorn", "app:app", "--host", "0.0.0.0", "--port", "8000"]

# ë¹Œë“œ & ì‹¤í–‰
docker build -t ai-tools .
docker run -p 8000:8000 --env-file .env ai-tools
```

### 7.3 ì‚¬ìš©ì í”¼ë“œë°± ë° ê°œì„ 

**í”¼ë“œë°± ìˆ˜ì§‘:**
```python
# feedback.py
"""ì‚¬ìš©ì í”¼ë“œë°± ìˆ˜ì§‘"""

class FeedbackCollector:
    """í”¼ë“œë°± ìˆ˜ì§‘ê¸°"""

    def collect_feedback(self, tool_name: str, output: str):
        """ì‚¬ìš©ì í‰ê°€ ìš”ì²­"""

        print("\n" + "="*50)
        print(f"ğŸ“‹ {tool_name} ì‚¬ìš© í›„ê¸°ë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”!")
        print("="*50)

        rating = input("ë§Œì¡±ë„ (1-5): ")
        comments = input("ê°œì„  ì‚¬í•­: ")

        feedback = {
            "tool": tool_name,
            "rating": rating,
            "comments": comments,
            "timestamp": datetime.now().isoformat()
        }

        # ì €ì¥ (íŒŒì¼, DB, ë˜ëŠ” ë¶„ì„ ë„êµ¬)
        with open("feedback.jsonl", 'a') as f:
            f.write(json.dumps(feedback, ensure_ascii=False) + "\n")

        print("âœ… í”¼ë“œë°± ê°ì‚¬í•©ë‹ˆë‹¤!")

# ì‚¬ìš© ì˜ˆì‹œ
collector = FeedbackCollector()
collector.collect_feedback("PRD Generator", prd_content)
```

---

**âœ… Phase 5 ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] ë³´ì•ˆ ì¡°ì¹˜ êµ¬í˜„
- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ë¹„ìš© ì¶”ì  ì‹œìŠ¤í…œ
- [ ] ë°°í¬ ì „ëµ ìˆ˜ë¦½
- [ ] í”¼ë“œë°± ë£¨í”„ êµ¬ì¶•

**ë‹¤ìŒ ë‹¨ê³„:** ì‹¤ìŠµ ì˜ˆì œë¡œ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ê²½í—˜í•©ë‹ˆë‹¤.

---

## 8. ì‹¤ìŠµ ì˜ˆì œ

### 8.1 ì¢…í•© ì‹¤ìŠµ: ìŠ¤íƒ€íŠ¸ì—… ìƒì‚°ì„± ë„êµ¬ êµ¬ì¶•

**ì‹œë‚˜ë¦¬ì˜¤:**
ë‹¹ì‹ ì€ 10ëª… ê·œëª¨ì˜ ìŠ¤íƒ€íŠ¸ì—…ì—ì„œ ì¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. PM, ê°œë°œì, ë””ìì´ë„ˆ ëª¨ë‘ê°€ AI ë„êµ¬ë¥¼ í™œìš©í•˜ì—¬ ìƒì‚°ì„±ì„ ë†’ì´ê³  ì‹¶ì–´í•©ë‹ˆë‹¤.

**êµ¬ì¶•í•  ë„êµ¬:**
1. PRD ìƒì„±ê¸°
2. íšŒì˜ë¡ ìš”ì•½ê¸°
3. ì½”ë“œ ë¦¬ë·° ë´‡

**ì‹¤ìŠµ ë‹¨ê³„:**

#### Step 1: í™˜ê²½ ì„¤ì • (10ë¶„)

```bash
# í”„ë¡œì íŠ¸ ìƒì„±
mkdir startup-ai-tools
cd startup-ai-tools

# Python í™˜ê²½
python -m venv venv
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install openai anthropic python-dotenv

# API í‚¤ ì„¤ì •
echo "OPENAI_API_KEY=your-key" > .env
echo "ANTHROPIC_API_KEY=your-key" >> .env
echo ".env" >> .gitignore
```

#### Step 2: PRD ìƒì„± (30ë¶„)

```bash
# ì´ ê°€ì´ë“œì˜ PRD ìƒì„±ê¸° ì½”ë“œ ì‚¬ìš©
cp /path/to/prd_generator.py .

# ì²« PRD ìƒì„±
python prd_generator.py \\
  --idea "íŒ€ ê°„ ì§€ì‹ ê³µìœ  í”Œë«í¼" \\
  --users "ìŠ¤íƒ€íŠ¸ì—… ì§ì›ë“¤" \\
  --goal "ì§€ì‹ ê²€ìƒ‰ ì‹œê°„ 50% ë‹¨ì¶•" \\
  --tech-stack "Next.js, FastAPI, PostgreSQL"

# ê²°ê³¼ ê²€í†  ë° í”¼ë“œë°±
```

#### Step 3: íšŒì˜ë¡ ìë™í™” (20ë¶„)

```bash
# íšŒì˜ ë…¹ìŒ (ì˜ˆì‹œ)
# ì‹¤ì œë¡œëŠ” Zoom/Google Meet ë…¹í™” ì‚¬ìš©

# í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ìš”ì•½
python meeting_summarizer.py \\
  --input weekly_meeting.txt \\
  --title "ì£¼ê°„ ìŠ¤í”„ë¦°íŠ¸ ë¦¬ë·°" \\
  --attendees "ê¹€PM,ì´ê°œë°œì,ë°•ë””ìì´ë„ˆ"
```

#### Step 4: ì½”ë“œ ë¦¬ë·° ìë™í™” (40ë¶„)

```bash
# GitHub PR ìƒì„±
git checkout -b feature/user-auth
# ... ì½”ë“œ ì‘ì„± ...
git push origin feature/user-auth
gh pr create

# AI ë¦¬ë·° ì‹¤í–‰
python code_reviewer.py --pr 1

# í”¼ë“œë°± í™•ì¸ ë° ìˆ˜ì •
```

#### Step 5: íŒ€ì— ê³µìœ  (10ë¶„)

```markdown
# README.md ì‘ì„±

# ìš°ë¦¬ íŒ€ AI ë„êµ¬

## ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬

### 1. PRD ìƒì„±ê¸°
**ìš©ë„:** ì‹ ê·œ ê¸°ëŠ¥ ë¬¸ì„œí™”
**ì‚¬ìš©ë²•:** `python prd_generator.py --idea "ê¸°ëŠ¥ ì•„ì´ë””ì–´"`

### 2. íšŒì˜ë¡ ìš”ì•½ê¸°
**ìš©ë„:** íšŒì˜ í›„ ìë™ ì •ë¦¬
**ì‚¬ìš©ë²•:** `python meeting_summarizer.py --input meeting.txt`

### 3. ì½”ë“œ ë¦¬ë·° ë´‡
**ìš©ë„:** PR ìë™ ë¦¬ë·°
**ì‚¬ìš©ë²•:** `python code_reviewer.py --pr PRë²ˆí˜¸`

## ì„¤ì •

1. Python 3.11+ ì„¤ì¹˜
2. `pip install -r requirements.txt`
3. `.env` íŒŒì¼ì— API í‚¤ ì„¤ì •
4. ì‚¬ìš©!

## ë¹„ìš©

- ì›” ì˜ˆìƒ: $50-100 (íŒ€ 10ëª… ê¸°ì¤€)
- ì ˆê° ì‹œê°„: ì£¼ë‹¹ 20ì‹œê°„
- ROI: 1000%+
```

### 8.2 ì¸¡ì • ë° ê°œì„ 

**1ì£¼ì°¨ í›„ ì¸¡ì •:**
```python
# metrics.py
"""ì„±ê³¼ ì¸¡ì •"""

def calculate_roi():
    """ROI ê³„ì‚°"""

    # ì‚¬ìš© í†µê³„
    prd_generated = 5
    meetings_summarized = 10
    prs_reviewed = 20

    # ì‹œê°„ ì ˆê° (ì‹œê°„)
    time_saved = (
        prd_generated * 4 +        # PRDë‹¹ 4ì‹œê°„ ì ˆê°
        meetings_summarized * 1 +  # íšŒì˜ë‹¹ 1ì‹œê°„
        prs_reviewed * 0.5         # PRë‹¹ 30ë¶„
    )

    # ë¹„ìš©
    ai_cost = 30  # ì›” AI API ë¹„ìš©
    hourly_rate = 50  # ì‹œê°„ë‹¹ ì¸ê±´ë¹„

    value_generated = time_saved * hourly_rate
    roi = ((value_generated - ai_cost) / ai_cost) * 100

    print(f"ğŸ“Š 1ì£¼ì°¨ ì„±ê³¼:")
    print(f"  - ì ˆê° ì‹œê°„: {time_saved}ì‹œê°„")
    print(f"  - ê°€ì¹˜ ì°½ì¶œ: ${value_generated}")
    print(f"  - AI ë¹„ìš©: ${ai_cost}")
    print(f"  - ROI: {roi:.0f}%")

calculate_roi()
```

**ê²°ê³¼:**
```
ğŸ“Š 1ì£¼ì°¨ ì„±ê³¼:
  - ì ˆê° ì‹œê°„: 35ì‹œê°„
  - ê°€ì¹˜ ì°½ì¶œ: $1750
  - AI ë¹„ìš©: $30
  - ROI: 5733%
```

---

## 9. ë¶€ë¡

### 9.1 ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ (FAQ)

**Q1: AIê°€ ìƒì„±í•œ ì½”ë“œ/ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ë„ ë˜ë‚˜ìš”?**
A: ì•„ë‹ˆìš”. í•­ìƒ ê²€í†  ë° ìˆ˜ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. AIëŠ” "ì´ˆì•ˆ ì‘ì„±ì"ì´ë©°, ìµœì¢… ê²°ì •ì€ ì‚¬ëŒì´ í•´ì•¼ í•©ë‹ˆë‹¤.

**Q2: ë¹„ìš©ì´ ë„ˆë¬´ ë§ì´ ë‚˜ì˜¤ë©´ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?**
A: (1) ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš© (2) ìºì‹± í™œìš© (3) ë°°ì¹˜ ì²˜ë¦¬ (4) í”„ë¡¬í”„íŠ¸ ìµœì í™”

**Q3: API í‚¤ê°€ ìœ ì¶œë˜ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?**
A: ì¦‰ì‹œ í‚¤ë¥¼ íê¸°í•˜ê³  ìƒˆë¡œ ë°œê¸‰ë°›ìœ¼ì„¸ìš”. Usage limits ì„¤ì •ìœ¼ë¡œ í”¼í•´ ìµœì†Œí™”.

**Q4: íŒ€ì›ë“¤ì´ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´?**
A: (1) ì„±ê³¼ ê³µìœ  (2) ê°„ë‹¨í•œ ë„êµ¬ë¶€í„° ì‹œì‘ (3) ì§ì ‘ ì‹œì—° (4) í”¼ë“œë°± ë°˜ì˜

**Q5: ì–´ë–¤ AI ì œê³µìë¥¼ ì„ íƒí•´ì•¼ í•˜ë‚˜ìš”?**
A:
- ë²”ìš©: OpenAI GPT-4
- ê¸´ ë¬¸ì„œ: Claude
- ë©€í‹°ëª¨ë‹¬: Gemini
- ë¹„ìš© ì¤‘ì‹œ: GPT-3.5-turbo

### 9.2 ì¶”ê°€ í•™ìŠµ ìë£Œ

**ê³µì‹ ë¬¸ì„œ:**
- OpenAI API: https://platform.openai.com/docs
- Anthropic Claude: https://docs.anthropic.com
- Google Gemini: https://ai.google.dev

**ì»¤ë®¤ë‹ˆí‹°:**
- r/PromptEngineering
- r/ChatGPTCoding
- Discord: OpenAI Developers

**ë„ì„œ:**
- "Prompt Engineering Guide" (DAIR.AI)
- "Building LLM Apps" (O'Reilly)

### 9.3 ì²´í¬ë¦¬ìŠ¤íŠ¸: í”„ë¡œë•ì…˜ ì¤€ë¹„

```markdown
## í”„ë¡œë•ì…˜ ë°°í¬ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### ë³´ì•ˆ
- [ ] API í‚¤ í™˜ê²½ ë³€ìˆ˜í™”
- [ ] .gitignoreì— .env ì¶”ê°€
- [ ] Rate limiting êµ¬í˜„
- [ ] ì…ë ¥ ê²€ì¦ ì¶”ê°€
- [ ] ë¡œê·¸ì—ì„œ ë¯¼ê° ì •ë³´ ì œê±°

### ì„±ëŠ¥
- [ ] ìºì‹± êµ¬í˜„
- [ ] íƒ€ì„ì•„ì›ƒ ì„¤ì •
- [ ] ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”
- [ ] ì¬ì‹œë„ ë¡œì§ ì¶”ê°€

### ëª¨ë‹ˆí„°ë§
- [ ] ì‚¬ìš©ëŸ‰ ì¶”ì 
- [ ] ë¹„ìš© ëª¨ë‹ˆí„°ë§
- [ ] ì—ëŸ¬ ë¡œê¹…
- [ ] ì„±ëŠ¥ ì§€í‘œ ìˆ˜ì§‘

### ë¬¸ì„œí™”
- [ ] README ì‘ì„±
- [ ] ì‚¬ìš© ì˜ˆì‹œ ì¶”ê°€
- [ ] API ë¬¸ì„œ ìƒì„±
- [ ] íŠ¸ëŸ¬ë¸”ìŠˆíŒ… ê°€ì´ë“œ

### í…ŒìŠ¤íŠ¸
- [ ] Unit test ì‘ì„±
- [ ] Integration test
- [ ] ì‹¤ì‚¬ìš©ì í…ŒìŠ¤íŠ¸
- [ ] í”¼ë“œë°± ìˆ˜ì§‘ ì²´ê³„

### ë°°í¬
- [ ] CI/CD íŒŒì´í”„ë¼ì¸
- [ ] Docker ì´ë¯¸ì§€
- [ ] ë°±ì—… ì „ëµ
- [ ] ë¡¤ë°± ê³„íš
```

### 9.4 ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

**ë¬¸ì œ: Rate Limit ì—ëŸ¬**
```python
# í•´ê²°: ì§€ìˆ˜ ë°±ì˜¤í”„ ì¬ì‹œë„
import time

for attempt in range(3):
    try:
        response = ai_call(prompt)
        break
    except RateLimitError:
        wait_time = 2 ** attempt
        print(f"Rate limited. Waiting {wait_time}s...")
        time.sleep(wait_time)
```

**ë¬¸ì œ: ì‘ë‹µì´ ë„ˆë¬´ ëŠë¦¼**
```python
# í•´ê²°: íƒ€ì„ì•„ì›ƒ ì„¤ì • + ìŠ¤íŠ¸ë¦¬ë°
response = client.chat.completions.create(
    model="gpt-4",
    messages=[...],
    timeout=30,  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
    stream=True  # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
)

for chunk in response:
    print(chunk.choices[0].delta.content, end="")
```

**ë¬¸ì œ: ë¹„ìš©ì´ ì˜ˆìƒë³´ë‹¤ ë§ì´ ë‚˜ì˜´**
```python
# í•´ê²°: ë¹„ìš© ì¶”ì  ë° ì•Œë¦¼
def track_cost(tokens: int, model: str):
    cost = calculate_cost(tokens, model)

    # ì¼ì¼ í•œë„ ì²´í¬
    daily_total = get_daily_total()
    if daily_total + cost > DAILY_LIMIT:
        raise Exception(f"Daily limit exceeded: ${daily_total + cost}")

    log_cost(cost)
```

---

## ë§ˆë¬´ë¦¬

ì´ ê°€ì´ë“œë¥¼ í†µí•´ AI ë„êµ¬ ê°œë°œì˜ ì „ ê³¼ì •ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤:

**í•™ìŠµí•œ ë‚´ìš©:**
- âœ… AI API ê¸°ì´ˆ ë° í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
- âœ… ì‹¤ì œ ì„±ê³µ ì‚¬ë¡€ ë¶„ì„ (Gazelle, Domina, Croud, ChatPRD)
- âœ… ê¸°íšììš© ë„êµ¬ (PRD, íšŒì˜ë¡, ì‚¬ìš©ì ìŠ¤í† ë¦¬)
- âœ… ê°œë°œììš© ë„êµ¬ (ì½”ë“œ ë¦¬ë·°, í…ŒìŠ¤íŠ¸, API ë¬¸ì„œ)
- âœ… í”„ë¡œë•ì…˜ ë°°í¬ ë° ìš´ì˜

**ë‹¤ìŒ ë‹¨ê³„:**
1. ê°€ì¥ ê°„ë‹¨í•œ ë„êµ¬ë¶€í„° êµ¬ì¶• ì‹œì‘
2. íŒ€ì› 1-2ëª…ê³¼ í•¨ê»˜ í…ŒìŠ¤íŠ¸
3. í”¼ë“œë°± ìˆ˜ì§‘ ë° ê°œì„ 
4. ì„±ê³¼ ì¸¡ì • ë° í™•ëŒ€

**í•µì‹¬ ì›ì¹™:**
- AIëŠ” ë„ìš°ë¯¸, ìµœì¢… ê²°ì •ì€ ì‚¬ëŒ
- ì‘ì€ ì„±ê³µë¶€í„° ì‹œì‘
- ì§€ì†ì ì¸ ê°œì„ 
- ë¹„ìš© ëŒ€ë¹„ ê°€ì¹˜ ì¶”ì 

**ì„±ê³µì„ ê¸°ì›í•©ë‹ˆë‹¤!** ğŸš€

---

**ë¬¸ì„œ ì •ë³´:**
- ë²„ì „: 1.0
- ìµœì¢… ìˆ˜ì •: 2026-01-15
- ì‘ì„±ì: AI Productivity Guide Team
- ë¼ì´ì„ ìŠ¤: MIT
